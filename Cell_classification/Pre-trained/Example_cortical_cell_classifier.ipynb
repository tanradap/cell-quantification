{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cortical classifier on novel slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load important libraries\n",
    "import sys\n",
    "sys.path.insert(0,\n",
    "                '/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Cell_pipeline/Cell_classification/')\n",
    "from base import *\n",
    "from constants import *\n",
    "import joblib "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load cortical classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Cell_pipeline/Cell_classification/Models/\"\n",
    "filename = \"cortical_cell_classifier.sav\"\n",
    "model = joblib.load(path+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (0.5052076758986506,\n",
       "  0.5843123101910119,\n",
       "  0.5443412365069623,\n",
       "  0.7156862745098039),\n",
       " 1: (0.5122703624147519,\n",
       "  0.9112137156584529,\n",
       "  0.9126933052503394,\n",
       "  0.9105209397344227),\n",
       " 2: (0.2828375714157505,\n",
       "  0.8576644007790974,\n",
       "  0.832396168021168,\n",
       "  0.892610759493671),\n",
       " 3: (0.28468038825335934,\n",
       "  0.7667554546504622,\n",
       "  0.7432463865928447,\n",
       "  0.8087519830777368)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nucleus: Max diameter µm</td>\n",
       "      <td>0.121259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nucleus: Area µm^2</td>\n",
       "      <td>0.113244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nucleus: Length µm</td>\n",
       "      <td>0.109141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Cell: Max diameter µm</td>\n",
       "      <td>0.108733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nucleus: Min diameter µm</td>\n",
       "      <td>0.085914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    features  importance\n",
       "5   Nucleus: Max diameter µm    0.121259\n",
       "1         Nucleus: Area µm^2    0.113244\n",
       "2         Nucleus: Length µm    0.109141\n",
       "11     Cell: Max diameter µm    0.108733\n",
       "6   Nucleus: Min diameter µm    0.085914"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.f_importance.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import files to make predictions on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in: 209  files\n",
      "Generated filename list for nb: 209 nb files\n",
      "Equal number of files?: True\n"
     ]
    }
   ],
   "source": [
    "# Data file \n",
    "with open(\"C:/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Cell_pipeline/Metadata/cortical_stardist.txt\") as f: \n",
    "    mylist= f.read().splitlines()\n",
    "    \n",
    "print(\"Read in:\",len(mylist),\" files\")\n",
    "\n",
    "# NB files \n",
    "nb_mylist = [i[0:6]+'_all_neighbours.csv' for i in  mylist]\n",
    "\n",
    "# # Hema files \n",
    "# hema_mylist = [i[0:6]+'_hema.csv' for i in  mylist]\n",
    "\n",
    "print(\"Generated filename list for nb:\", len(nb_mylist), \"nb files\")\n",
    "print(\"Equal number of files?:\", (len(mylist)==len(nb_mylist)))\n",
    "\n",
    "# print(\"Generated filename list for hema:\", len(hema_mylist), \"hema files\")\n",
    "# print(\"Equal number of files?:\", (len(mylist)==len(nb_mylist))==(len(mylist)==len(hema_mylist)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process new files to make predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "755558 - cannot select centroid columns= fixed now.\n",
    "771762_all.txt - file not found\n",
    "771742_all.txt - file not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist.index(\"755558_all.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = [mylist[49]]\n",
    "nb_mylist = [nb_mylist[49]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['755558_all.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE 755558_all.txt Number:  1 / 1\n",
      "---------------STEP1: DATA FILE-------------------\n",
      "Read in data file: 755558_all.txt\n",
      "Data shape is: (490200, 41)\n",
      "----------------------------------------------------------------------------\n",
      "---------------STEP2: NB FILE-------------------\n",
      "Read in nb file: 755558_all_neighbours.csv\n",
      "nb_dat shape is: (490200, 12)\n",
      "-----------------------------------------\n",
      "Successfully combine nb cell counts to main data\n",
      "Expected & Observed row matched?:  True\n",
      "Expected & Observed columns matched?:  True\n",
      "----------------------------------------------------------------------------\n",
      "---------------STEP3: HEMA FILE-------------------\n",
      "hema_dat shape is: (490200, 3)\n",
      "----------------------------------------------------------------------------\n",
      "---------------STEP4: HEMA NORM-------------------\n",
      "No. of cells with normalised Hema >1: 4900 from 490200 detected cells\n",
      "755558_all.txt : To be removed = actually removed? True  & 4900 cells removed\n",
      "----------------------------------------------------------------------------\n",
      "---------------STEP5: CHECKING NA & REGIONS-------------------\n",
      "Shape of data, ready for prediction (225576, 51)\n",
      "----------------------------------------------------------------------------\n",
      "---------------STEP7: PREDICTIONS-------------------\n",
      "X_unlabelled shape:  (225576, 44)\n",
      "Neuron       71859\n",
      "Others       60816\n",
      "Oligo        49189\n",
      "Ambiguous    30700\n",
      "Astro        13012\n",
      "Name: Class, dtype: int64\n",
      "----------------------------------------------------------------------------\n",
      "---------------STEP8: DATA EXTRACTION & EXPORT-------------------\n",
      "Input data == output data?: True\n",
      "No NAN values in predicted portion?:  True\n",
      "Exported prediction of :  755558_all.txt\n",
      "----------------------------------------------------------------------------\n",
      "**********ALL DONE! YAY, no error!***********\n"
     ]
    }
   ],
   "source": [
    "n_total = len(mylist)\n",
    "faulty_file = []\n",
    "for i in range(0,n_total):\n",
    "    \n",
    "    # Read in unlabelled/unannotated file\n",
    "    print(\"FILE\", mylist[i], \"Number: \", i+1,\"/\",n_total)\n",
    "    print(\"---------------STEP1: DATA FILE-------------------\")\n",
    "    dat_file = mylist[i]\n",
    "    \n",
    "    dat_orig = pd.read_csv('C:/Users/mokur/OneDrive - University of Cambridge/Attachments/Jan2023/Cell_pipeline/detection_stardist/'+dat_file,sep=\"\\t\")\n",
    "    dat_orig.columns.values[5] = \"Centroid_X\"\n",
    "    dat_orig.columns.values[6] = \"Centroid_Y\"\n",
    "    dat_ = dat_orig[cell_extracted_features]\n",
    "    dab_df = dat_orig[dab_features]\n",
    "    dat = dat_.copy()\n",
    "    dat.loc[:,'Class'] = dat.shape[0]*['Unlabelled']\n",
    "    print(\"Read in data file:\", dat_file)\n",
    "    print(\"Data shape is:\", dat.shape)\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    \n",
    "    # Importing NB cells\n",
    "    print(\"---------------STEP2: NB FILE-------------------\")\n",
    "    nb_dat_file = nb_mylist[i]\n",
    "    nb_dat_ = pd.read_csv('E:/number_of_neighbours/'+nb_dat_file,sep=\",\")\n",
    "    nb_dat_ = nb_dat_.rename(columns={'X':'Centroid_X','Y':'Centroid_Y'})\n",
    "    nb_dat = nb_dat_[nnb_extracted_features]\n",
    "    \n",
    "    print(\"Read in nb file:\", nb_dat_file)\n",
    "    print(\"nb_dat shape is:\", nb_dat.shape)\n",
    "    print(\"-----------------------------------------\")\n",
    "    \n",
    "    ## Merge NB info to main data ***** From now on: Use 'combined'\n",
    "    combined = dat.merge(nb_dat.drop_duplicates(),on=['Centroid_X','Centroid_Y'],how='inner',validate='1:1')\n",
    "    print(\"Successfully combine nb cell counts to main data\")\n",
    "    print('Expected & Observed row matched?: ', dat.shape[0]==combined.shape[0])\n",
    "    print('Expected & Observed columns matched?: ', (dat.shape[1]+nb_dat.shape[1]-2)==combined.shape[1])\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    \n",
    "\n",
    "    # Importing hema files \n",
    "    print(\"---------------STEP3: HEMA FILE-------------------\")\n",
    "    hema_dat = dat[['Centroid_X','Centroid_Y','Hematoxylin: Nucleus: Mean']]\n",
    "    print(\"hema_dat shape is:\", hema_dat.shape)\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    \n",
    "    ############################  Normalising hema files ############################ \n",
    "    print(\"---------------STEP4: HEMA NORM-------------------\")\n",
    "    \n",
    "    # 1) Find cells need removing \n",
    "    hema_to_remove = find_hema_to_remove_slide(hema_dat)\n",
    "    print(\"No. of cells with normalised Hema >1:\",\n",
    "          len(hema_to_remove),\n",
    "            \"from\", len(hema_dat),\"detected cells\")\n",
    "\n",
    "    # 2) Remove hema from the slides \n",
    "    retained, removed_, remove_log = remove_cell_hema_slide(combined,\n",
    "                           hema_to_remove)\n",
    "    \n",
    "    print(dat_file,\":\",\n",
    "           'To be removed = actually removed?',remove_log,\n",
    "           ' &',removed_.shape[0],\"cells removed\")\n",
    "    \n",
    "    # label those cells we have removed as having Class = 'Excluded'\n",
    "    removed = removed_.copy()\n",
    "    removed.loc[:,'Class'] = ['Excluded']*removed.shape[0]\n",
    "\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "\n",
    "    ############################ Checking for NAs & Other bits  \n",
    "    print(\"---------------STEP5: CHECKING NA & REGIONS-------------------\")\n",
    "    \n",
    "    # 1) Selecting only GREY MATTER PORTION (since thresholding won't make sense in WM)\n",
    "    if (retained[retained['Name']=='Grey_matter'].shape[0] == 0): \n",
    "        faulty_file.append(retained['Image'][0])\n",
    "        continue  \n",
    "    \n",
    "    retained2 = retained[retained['Name']=='Grey_matter']  # only has GM now\n",
    "    retained_not_GM = retained[retained['Name']!='Grey_matter'] # Class will be Unlabelled\n",
    "    \n",
    "\n",
    "    # 2) Remove NA cells \n",
    "    retained3 = retained2.dropna()\n",
    "    NA_proportion = retained2[retained2.isna().any(axis=1)]\n",
    "    print(\"Shape of data, ready for prediction\", retained3.shape)\n",
    "\n",
    "\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    " \n",
    "    ############################ Prediction\n",
    "    print(\"---------------STEP7: PREDICTIONS-------------------\")\n",
    "    \n",
    "    # Create new variable for retained to add Class predictions to\n",
    "    retained_final = retained3.copy()\n",
    "\n",
    "    # Dropping extra info features \n",
    "    X_unlabelled = retained3.drop(columns=['Image',\n",
    "                                           'Name',\n",
    "                                           'Class',\n",
    "                                           'Parent',\n",
    "                                           'ROI',\n",
    "                                           'Centroid_X',\n",
    "                                           'Centroid_Y'])\n",
    "    print('X_unlabelled shape: ', X_unlabelled.shape)\n",
    "\n",
    "        \n",
    "    # 1) Perform prediction on the novel slide\n",
    "    model.predict(X_unlabelled)\n",
    "    retained_final.loc[:,'Class'] = model.prediction\n",
    "    print(retained_final['Class'].value_counts())\n",
    "    \n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    ############################ Extracting data out  ############################ \n",
    "    print(\"---------------STEP8: DATA EXTRACTION & EXPORT-------------------\")\n",
    "\n",
    "    # 1) Combining predicted cells & excluded cells (prior to prediction)\n",
    "    output_slide = pd.concat([removed, # from hema\n",
    "                              retained_not_GM, # non GM \n",
    "                              NA_proportion, # GM with NA \n",
    "                              retained_final]) # predicted GM portion\n",
    "    \n",
    "    # 2) Add DAB information \n",
    "    output_slide_dab = output_slide.merge(dab_df,on=['Centroid_X','Centroid_Y'])\n",
    "    \n",
    "    # 2) Checking input data == output data \n",
    "    print(\"Input data == output data?:\", combined.shape[0]==output_slide_dab.shape[0])\n",
    "\n",
    "    # 3) Checking if there are NA values in predicted portion after combining data \n",
    "    pred_complete=output_slide_dab[output_slide_dab['Class']!='Unlabelled'].isna().sum().sum()\n",
    "    print(\"No NAN values in predicted portion?: \", 0==pred_complete)\n",
    "    \n",
    "    # 4) Exporting relevant information \n",
    "    path = 'C:/Users/mokur/OneDrive/Desktop/Digital_path/Cell_pipeline/Predictions/Cortical/' + output_slide_dab.iloc[0,0]+'_predictions.txt'\n",
    "    output_slide_dab.to_csv(path, sep='\\t',index=False)\n",
    "    print(\"Exported prediction of : \",dat_file)\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"**********ALL DONE! YAY, no error!***********\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tau-pl-old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
