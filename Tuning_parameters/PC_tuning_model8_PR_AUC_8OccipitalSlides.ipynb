{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic classification: RF & adjust tuning step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thresholding in the context of:\n",
    "\n",
    "1) Compute the difference between p(class) - t(class) **\n",
    "\n",
    "1) Sorted in descending order\n",
    "\n",
    "2) Compare max to threshold, if less then we move on to compare the next most likely class against the threshold\n",
    "\n",
    "3) Continue until probability is equal or greater than threshold, assign that class\n",
    "\n",
    "4) If no classes get assigned, then that cell is labelled as 'ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import random \n",
    "\n",
    "from sklearn import preprocessing \n",
    "from statistics import mean\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "from pprint import pprint \n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "#import ml_insights as mli "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Require: \n",
    "1. input_files.txt - to contian filenames I want to use. ** currently .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in:  8 files\n",
      "721703_cell_annotations.csv  number of features:  62\n",
      "721770_cell_annotations.csv  number of features:  61\n",
      "747377_cell_annotations.csv  number of features:  342\n",
      "747813_cell_annotations.csv  number of features:  342\n",
      "747847_cell_annotations.csv  number of features:  342\n",
      "747848_cell_annotations.csv  number of features:  342\n",
      "755504_cell_annotations.csv  number of features:  61\n",
      "755508_cell_annotations.csv  number of features:  61\n",
      "Extracted: 8 files\n",
      "Note: inconsistencies come from tau necrosis - will be removed later\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Name</th>\n",
       "      <th>Class</th>\n",
       "      <th>Parent</th>\n",
       "      <th>ROI</th>\n",
       "      <th>Centroid_X</th>\n",
       "      <th>Centroid_Y</th>\n",
       "      <th>Detection probability</th>\n",
       "      <th>Nucleus: Area ¬µm^2</th>\n",
       "      <th>Nucleus: Length ¬µm</th>\n",
       "      <th>...</th>\n",
       "      <th>DAB: Membrane: Mean</th>\n",
       "      <th>DAB: Membrane: Median</th>\n",
       "      <th>DAB: Membrane: Min</th>\n",
       "      <th>DAB: Membrane: Max</th>\n",
       "      <th>DAB: Membrane: Std.Dev.</th>\n",
       "      <th>DAB: Cell: Mean</th>\n",
       "      <th>DAB: Cell: Median</th>\n",
       "      <th>DAB: Cell: Min</th>\n",
       "      <th>DAB: Cell: Max</th>\n",
       "      <th>DAB: Cell: Std.Dev.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>721770.svs</td>\n",
       "      <td>Grey_matter</td>\n",
       "      <td>Unlabelled</td>\n",
       "      <td>PathAnnotationObject</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>6724.3</td>\n",
       "      <td>1282.3</td>\n",
       "      <td>0.8278</td>\n",
       "      <td>16.3984</td>\n",
       "      <td>15.1591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>-0.0285</td>\n",
       "      <td>0.2166</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>-0.0989</td>\n",
       "      <td>0.5054</td>\n",
       "      <td>0.0768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>721770.svs</td>\n",
       "      <td>Grey_matter</td>\n",
       "      <td>Unlabelled</td>\n",
       "      <td>PathAnnotationObject</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>6667.9</td>\n",
       "      <td>1293.1</td>\n",
       "      <td>0.7811</td>\n",
       "      <td>22.6658</td>\n",
       "      <td>17.4292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>-0.0270</td>\n",
       "      <td>0.2093</td>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.0646</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>-0.1584</td>\n",
       "      <td>0.3228</td>\n",
       "      <td>0.0645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>721770.svs</td>\n",
       "      <td>Grey_matter</td>\n",
       "      <td>Unlabelled</td>\n",
       "      <td>PathAnnotationObject</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>6696.7</td>\n",
       "      <td>1293.7</td>\n",
       "      <td>0.8021</td>\n",
       "      <td>36.8686</td>\n",
       "      <td>21.9999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.0429</td>\n",
       "      <td>-0.0138</td>\n",
       "      <td>0.2803</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.0643</td>\n",
       "      <td>0.0358</td>\n",
       "      <td>-0.1187</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.0766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>721770.svs</td>\n",
       "      <td>Grey_matter</td>\n",
       "      <td>Unlabelled</td>\n",
       "      <td>PathAnnotationObject</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>6732.0</td>\n",
       "      <td>1295.1</td>\n",
       "      <td>0.8091</td>\n",
       "      <td>17.6270</td>\n",
       "      <td>15.9432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>-0.0358</td>\n",
       "      <td>0.2388</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>-0.1247</td>\n",
       "      <td>0.3948</td>\n",
       "      <td>0.0627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>721770.svs</td>\n",
       "      <td>Grey_matter</td>\n",
       "      <td>Unlabelled</td>\n",
       "      <td>PathAnnotationObject</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>6928.0</td>\n",
       "      <td>1302.2</td>\n",
       "      <td>0.5366</td>\n",
       "      <td>19.3119</td>\n",
       "      <td>17.7394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>-0.0114</td>\n",
       "      <td>0.1927</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0504</td>\n",
       "      <td>0.0419</td>\n",
       "      <td>-0.0355</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image         Name       Class                Parent      ROI  \\\n",
       "0  721770.svs  Grey_matter  Unlabelled  PathAnnotationObject  Polygon   \n",
       "1  721770.svs  Grey_matter  Unlabelled  PathAnnotationObject  Polygon   \n",
       "2  721770.svs  Grey_matter  Unlabelled  PathAnnotationObject  Polygon   \n",
       "3  721770.svs  Grey_matter  Unlabelled  PathAnnotationObject  Polygon   \n",
       "4  721770.svs  Grey_matter  Unlabelled  PathAnnotationObject  Polygon   \n",
       "\n",
       "   Centroid_X  Centroid_Y  Detection probability  Nucleus: Area ¬µm^2  \\\n",
       "0      6724.3      1282.3                 0.8278              16.3984   \n",
       "1      6667.9      1293.1                 0.7811              22.6658   \n",
       "2      6696.7      1293.7                 0.8021              36.8686   \n",
       "3      6732.0      1295.1                 0.8091              17.6270   \n",
       "4      6928.0      1302.2                 0.5366              19.3119   \n",
       "\n",
       "   Nucleus: Length ¬µm  ...  DAB: Membrane: Mean  DAB: Membrane: Median  \\\n",
       "0              15.1591  ...               0.0376                 0.0300   \n",
       "1              17.4292  ...               0.0484                 0.0304   \n",
       "2              21.9999  ...               0.0545                 0.0429   \n",
       "3              15.9432  ...               0.0422                 0.0273   \n",
       "4              17.7394  ...               0.0547                 0.0460   \n",
       "\n",
       "   DAB: Membrane: Min  DAB: Membrane: Max  DAB: Membrane: Std.Dev.  \\\n",
       "0             -0.0285              0.2166                   0.0383   \n",
       "1             -0.0270              0.2093                   0.0482   \n",
       "2             -0.0138              0.2803                   0.0494   \n",
       "3             -0.0358              0.2388                   0.0472   \n",
       "4             -0.0114              0.1927                   0.0428   \n",
       "\n",
       "   DAB: Cell: Mean  DAB: Cell: Median  DAB: Cell: Min  DAB: Cell: Max  \\\n",
       "0           0.0616             0.0324         -0.0989          0.5054   \n",
       "1           0.0646             0.0502         -0.1584          0.3228   \n",
       "2           0.0643             0.0358         -0.1187          0.4674   \n",
       "3           0.0649             0.0459         -0.1247          0.3948   \n",
       "4           0.0504             0.0419         -0.0355          0.2131   \n",
       "\n",
       "   DAB: Cell: Std.Dev.  \n",
       "0               0.0768  \n",
       "1               0.0645  \n",
       "2               0.0766  \n",
       "3               0.0627  \n",
       "4               0.0401  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 1) Importing annotated cells \n",
    "#Variables: mylist, inputs \n",
    "\n",
    "## obtaining list of files \n",
    "with open(\"D:/Tanrada_classification/imbalance_cortical_training/occipital/annotated/training_slides.txt\") as f: \n",
    "    mylist= f.read().splitlines()\n",
    "    \n",
    "print(\"Read in: \",len(mylist),\"files\")\n",
    "\n",
    "## 2) reading in all those files \n",
    "inputs = [] \n",
    "for i in mylist: \n",
    "    dat = pd.read_csv('D:/Tanrada_classification/imbalance_cortical_training/occipital/annotated/'+i,sep=\",\")\n",
    "    ## Changing column names - since these names tend to be inconsistent causing problems \n",
    "    dat.columns.values[5] = 'Centroid_X'\n",
    "    dat.columns.values[6] = 'Centroid_Y'\n",
    "    dat.columns.values[8] = 'Nucleus: Area ¬µm^2'\n",
    "    dat.columns.values[9] = 'Nucleus: Length ¬µm'\n",
    "    dat.columns.values[12] = 'Nucleus: Max diameter ¬µm'\n",
    "    dat.columns.values[13] = 'Nucleus: Min diameter ¬µm'\n",
    "    dat.columns.values[14] = 'Cell: Area ¬µm^2'\n",
    "    dat.columns.values[15] = 'Cell: Length ¬µm'\n",
    "    dat.columns.values[18] = 'Cell: Max diameter ¬µm'\n",
    "    dat.columns.values[19] = 'Cell: Min diameter ¬µm'\n",
    "    #dat_cleaned = dat.iloc[:,0:61] ## SELECTING ONLY RELELVANT COLUMNS \n",
    "    print(i,\" number of features: \", dat.shape[1])\n",
    "    inputs.append(dat)\n",
    "\n",
    "print(\"Extracted:\", len(inputs),\"files\")\n",
    "print(\"Note: inconsistencies come from tau necrosis - will be removed later\")\n",
    "#Example\n",
    "inputs[1].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in: 8  NUMBER OF neighbouring cells files\n",
      "Extracted: 8 files\n"
     ]
    }
   ],
   "source": [
    "# 2.5) Importing in neghbouring cells info (numbers)\n",
    "\n",
    "nb_mylist = [i[0:6]+'_all_neighbours.csv' for i in mylist]\n",
    "print(\"Read in:\",len(nb_mylist),\" NUMBER OF neighbouring cells files\")\n",
    "\n",
    "# reading in all those files \n",
    "nb_inputs = [] \n",
    "for i in nb_mylist: \n",
    "    dat = pd.read_csv(\"D:/number_of_neighbours/\"+i,sep=\",\")\n",
    "    nb_inputs.append(dat)\n",
    "    \n",
    "print(\"Extracted:\", len(nb_inputs),\"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features  18\n",
      "Number of features  18\n",
      "Number of features  18\n",
      "Number of features  18\n",
      "Number of features  18\n",
      "Number of features  18\n",
      "Number of features  18\n",
      "Number of features  18\n"
     ]
    }
   ],
   "source": [
    "# Inspecting number of columns in NNB files: \n",
    "for i in nb_inputs:\n",
    "    print(\"Number of features \",i.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_dat shape: (424899, 12)\n",
      "dat shape: (424899, 62)\n",
      "Expected shape: 424899 72  Resulting shape: (424899, 72)\n",
      "--------------------------\n",
      "nb_dat shape: (208830, 12)\n",
      "dat shape: (208830, 61)\n",
      "Expected shape: 208830 71  Resulting shape: (208830, 71)\n",
      "--------------------------\n",
      "nb_dat shape: (232377, 12)\n",
      "dat shape: (174, 342)\n",
      "Expected shape: 174 352  Resulting shape: (174, 352)\n",
      "--------------------------\n",
      "nb_dat shape: (544006, 12)\n",
      "dat shape: (159, 342)\n",
      "Expected shape: 159 352  Resulting shape: (159, 352)\n",
      "--------------------------\n",
      "nb_dat shape: (841022, 12)\n",
      "dat shape: (176, 342)\n",
      "Expected shape: 176 352  Resulting shape: (176, 352)\n",
      "--------------------------\n",
      "nb_dat shape: (343097, 12)\n",
      "dat shape: (178, 342)\n",
      "Expected shape: 178 352  Resulting shape: (178, 352)\n",
      "--------------------------\n",
      "nb_dat shape: (231371, 12)\n",
      "dat shape: (231371, 61)\n",
      "Expected shape: 231371 71  Resulting shape: (231371, 71)\n",
      "--------------------------\n",
      "nb_dat shape: (176168, 12)\n",
      "dat shape: (176168, 61)\n",
      "Expected shape: 176168 71  Resulting shape: (176168, 71)\n",
      "--------------------------\n",
      "Succesfully combined nb cell counts to main data\n"
     ]
    }
   ],
   "source": [
    "## Extracting only annotated cells from nb files \n",
    "inputs_=[]\n",
    "n=len(inputs)\n",
    "for i in range(0,n):\n",
    "    #all cells on slide\n",
    "    nb_dat_ = nb_inputs[i]\n",
    "    nb_dat_ = nb_dat_.rename(columns={'X':'Centroid_X','Y':'Centroid_Y'})\n",
    "    nb_dat = nb_dat_[['Centroid_X','Centroid_Y','NN_10_um','NN_20_um','NN_30_um','NN_40_um','NN_50_um'\n",
    "                    ,'NN_60_um','NN_70_um','NN_80_um','NN_90_um','NN_100_um']] #so we have removed 'slice_id' or 'Image'\n",
    "    print(\"nb_dat shape:\", nb_dat.shape)\n",
    "    #annotated cells with no nb info\n",
    "    dat = inputs[i]\n",
    "    print(\"dat shape:\", dat.shape)\n",
    "    \n",
    "    #annotated cells with nb info: intersect between 2 dataframes \n",
    "    combined = dat.merge(nb_dat,on=['Centroid_X','Centroid_Y'],how='inner',validate='1:1') \n",
    "    inputs_.append(combined)\n",
    "    print(\"Expected shape:\", dat.shape[0],dat.shape[1]+nb_dat.shape[1]-2,\" Resulting shape:\",combined.shape)\n",
    "    print(\"--------------------------\")\n",
    "    \n",
    "print(\"Succesfully combined nb cell counts to main data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in: 8 hema files\n",
      "Extracted: 8 hema files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Centroid_X</th>\n",
       "      <th>Centroid_Y</th>\n",
       "      <th>Hematoxylin: Nucleus: Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4029.3</td>\n",
       "      <td>1035.7</td>\n",
       "      <td>0.4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4035.5</td>\n",
       "      <td>1037.4</td>\n",
       "      <td>0.6438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4046.0</td>\n",
       "      <td>1039.4</td>\n",
       "      <td>0.6428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4132.2</td>\n",
       "      <td>1039.4</td>\n",
       "      <td>0.6158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3883.3</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.6310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Centroid_X  Centroid_Y  Hematoxylin: Nucleus: Mean\n",
       "0      4029.3      1035.7                      0.4228\n",
       "1      4035.5      1037.4                      0.6438\n",
       "2      4046.0      1039.4                      0.6428\n",
       "3      4132.2      1039.4                      0.6158\n",
       "4      3883.3      1040.0                      0.6310"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 2) Importing hema nucleus mean of all detected cells & location coordinates \n",
    "# Variables: hema_mylist, hema_inputs \n",
    "\n",
    "hema_mylist = [i[0:6]+'_hema.csv' for i in mylist]    \n",
    "print(\"Read in:\",len(hema_mylist),\"hema files\")    \n",
    "\n",
    "\n",
    "## 4) reading in all those files \n",
    "hema_inputs = [] \n",
    "for i in hema_mylist: \n",
    "    dat = pd.read_csv('D:/Tanrada_classification/hema_novel/'+i,sep=\",\")\n",
    "    dat.columns.values[0] = 'Centroid_X' # To fix naming inconsistency problem \n",
    "    dat.columns.values[1] = 'Centroid_Y'\n",
    "    hema_inputs.append(dat)\n",
    "\n",
    "print(\"Extracted:\",len(hema_inputs),\"hema files\")  \n",
    "\n",
    "#Example \n",
    "hema_inputs[2].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mylist & nb_list matched?: True\n",
      "mylist & hema_list matched?: True\n"
     ]
    }
   ],
   "source": [
    "# Checking if filenames & order of them from mylist, nb_mylist & hema_mylist match\n",
    "x_nb = [i[0:6] for i in nb_mylist]\n",
    "x = [i[0:6] for i in mylist]\n",
    "x_h = [i[0:6] for i in hema_mylist]\n",
    "print(\"mylist & nb_list matched?:\", x==x_nb)\n",
    "print(\"mylist & hema_list matched?:\",x==x_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising hematoxlyin per brain side & discard top 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  No. of cells with normalised Hema >1: 4245 from 424899 detected cells\n",
      "1  No. of cells with normalised Hema >1: 2089 from 208830 detected cells\n",
      "2  No. of cells with normalised Hema >1: 2324 from 232377 detected cells\n",
      "3  No. of cells with normalised Hema >1: 5440 from 544006 detected cells\n",
      "4  No. of cells with normalised Hema >1: 8388 from 841022 detected cells\n",
      "5  No. of cells with normalised Hema >1: 3431 from 343097 detected cells\n",
      "6  No. of cells with normalised Hema >1: 2311 from 231371 detected cells\n",
      "7  No. of cells with normalised Hema >1: 1758 from 176168 detected cells\n"
     ]
    }
   ],
   "source": [
    "### 1) Get instances needed to be remove for each slide\n",
    "#Variables: hema_to_remove, hema_inputs \n",
    "hema_to_remove = [] \n",
    "for h in hema_inputs: \n",
    "    h2 = h.copy() \n",
    "    hema = h2['Hematoxylin: Nucleus: Mean']\n",
    "    threshold = hema.quantile(0.99)\n",
    "    hema_norm = hema/threshold \n",
    "    h2['Hematoxylin: Nucleus: Mean'] = hema_norm \n",
    "    h2 = h2[h2['Hematoxylin: Nucleus: Mean']>1] # to select instances need removing (keep <=1)\n",
    "    hema_to_remove.append(h2)\n",
    "\n",
    "for i in range(0,len(hema_to_remove)): \n",
    "    print(i, \" No. of cells with normalised Hema >1:\",len(hema_to_remove[i]), \"from\", len(hema_inputs[i]),\"detected cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721703_cell_annotations.csv : 4245 cells removed\n",
      "721770_cell_annotations.csv : 2089 cells removed\n",
      "747377_cell_annotations.csv : 0 cells removed\n",
      "747813_cell_annotations.csv : 2 cells removed\n",
      "747847_cell_annotations.csv : 7 cells removed\n",
      "747848_cell_annotations.csv : 0 cells removed\n",
      "755504_cell_annotations.csv : 2311 cells removed\n",
      "755508_cell_annotations.csv : 1758 cells removed\n"
     ]
    }
   ],
   "source": [
    "## 2) Discarding annotated cells if they fit the criteria above \n",
    "#Variables: cleaned_inputs, removed  \n",
    "\n",
    "cleaned_inputs = []\n",
    "removed = [] \n",
    "for n in range(0,(len(inputs_))): #looping through annotated % hema (detected) slides \n",
    "    \n",
    "    i = inputs_[n] #annotated cells \n",
    "    h = hema_to_remove[n] #cells we need to remove, may or may not contain annotated cells \n",
    "    \n",
    "    #Find cells that exist in both 'i' & 'h' = cells we want to remove \n",
    "    to_remove = i.merge(h,on=['Centroid_X','Centroid_Y'],how='inner',validate='1:1')\n",
    "    \n",
    "    #Find cells that only exist in 'i' but not in 'h' = cells we want to retain \n",
    "    to_retain = i.merge(h,on=['Centroid_X','Centroid_Y'],how='left',indicator=True,validate='1:1')\n",
    "    \n",
    "    #Extract cells we want to retain \n",
    "    retained = i[to_retain['_merge']=='left_only']\n",
    "    \n",
    "    cleaned_inputs.append(retained)\n",
    "    removed.append(to_remove)\n",
    "    print(mylist[n],\":\",i.shape[0]-retained.shape[0],\"cells removed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420654, 72)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Image',\n",
       " 'Name',\n",
       " 'Class',\n",
       " 'Parent',\n",
       " 'ROI',\n",
       " 'Centroid_X',\n",
       " 'Centroid_Y',\n",
       " 'Detection probability',\n",
       " 'Nucleus: Area ¬µm^2',\n",
       " 'Nucleus: Length ¬µm',\n",
       " 'Nucleus: Circularity',\n",
       " 'Nucleus: Solidity',\n",
       " 'Nucleus: Max diameter ¬µm',\n",
       " 'Nucleus: Min diameter ¬µm',\n",
       " 'Cell: Area ¬µm^2',\n",
       " 'Cell: Length ¬µm',\n",
       " 'Cell: Circularity',\n",
       " 'Cell: Solidity',\n",
       " 'Cell: Max diameter ¬µm',\n",
       " 'Cell: Min diameter ¬µm',\n",
       " 'Nucleus/Cell area ratio',\n",
       " 'Hematoxylin: Nucleus: Mean',\n",
       " 'Hematoxylin: Nucleus: Median',\n",
       " 'Hematoxylin: Nucleus: Min',\n",
       " 'Hematoxylin: Nucleus: Max',\n",
       " 'Hematoxylin: Nucleus: Std.Dev.',\n",
       " 'Hematoxylin: Cytoplasm: Mean',\n",
       " 'Hematoxylin: Cytoplasm: Median',\n",
       " 'Hematoxylin: Cytoplasm: Min',\n",
       " 'Hematoxylin: Cytoplasm: Max',\n",
       " 'Hematoxylin: Cytoplasm: Std.Dev.',\n",
       " 'Hematoxylin: Membrane: Mean',\n",
       " 'Hematoxylin: Membrane: Median',\n",
       " 'Hematoxylin: Membrane: Min',\n",
       " 'Hematoxylin: Membrane: Max',\n",
       " 'Hematoxylin: Membrane: Std.Dev.',\n",
       " 'Hematoxylin: Cell: Mean',\n",
       " 'Hematoxylin: Cell: Median',\n",
       " 'Hematoxylin: Cell: Min',\n",
       " 'Hematoxylin: Cell: Max',\n",
       " 'Hematoxylin: Cell: Std.Dev.',\n",
       " 'DAB: Nucleus: Mean',\n",
       " 'DAB: Nucleus: Median',\n",
       " 'DAB: Nucleus: Min',\n",
       " 'DAB: Nucleus: Max',\n",
       " 'DAB: Nucleus: Std.Dev.',\n",
       " 'DAB: Cytoplasm: Mean',\n",
       " 'DAB: Cytoplasm: Median',\n",
       " 'DAB: Cytoplasm: Min',\n",
       " 'DAB: Cytoplasm: Max',\n",
       " 'DAB: Cytoplasm: Std.Dev.',\n",
       " 'DAB: Membrane: Mean',\n",
       " 'DAB: Membrane: Median',\n",
       " 'DAB: Membrane: Min',\n",
       " 'DAB: Membrane: Max',\n",
       " 'DAB: Membrane: Std.Dev.',\n",
       " 'DAB: Cell: Mean',\n",
       " 'DAB: Cell: Median',\n",
       " 'DAB: Cell: Min',\n",
       " 'DAB: Cell: Max',\n",
       " 'DAB: Cell: Std.Dev.',\n",
       " 'tau: Necrosis area µm^2',\n",
       " 'NN_10_um',\n",
       " 'NN_20_um',\n",
       " 'NN_30_um',\n",
       " 'NN_40_um',\n",
       " 'NN_50_um',\n",
       " 'NN_60_um',\n",
       " 'NN_70_um',\n",
       " 'NN_80_um',\n",
       " 'NN_90_um',\n",
       " 'NN_100_um']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cleaned_inputs[0].shape)\n",
    "list(cleaned_inputs[0].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing DAB & tau necrosis & Image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial n_features: 72 , After removal: 51\n",
      "Initial n_features: 71 , After removal: 51\n",
      "Initial n_features: 352 , After removal: 51\n",
      "Initial n_features: 352 , After removal: 51\n",
      "Initial n_features: 352 , After removal: 51\n",
      "Initial n_features: 352 , After removal: 51\n",
      "Initial n_features: 71 , After removal: 51\n",
      "Initial n_features: 71 , After removal: 51\n"
     ]
    }
   ],
   "source": [
    "## Removing DAB & tau necrosis ** making sure same dimension\n",
    "cleaned_inputs_ = []\n",
    "for i in cleaned_inputs:\n",
    "    # To remove all DAB features\n",
    "    to_drop1 = list(i.filter(regex='DAB'))\n",
    "    dat1 = i[i.columns.drop(to_drop1)]\n",
    "    # To remove tau necrosis features\n",
    "    to_drop2 = list(dat1.filter(regex='tau'))\n",
    "    dat2= dat1[dat1.columns.drop(to_drop2)]\n",
    "    # To remove Smoothed ****\n",
    "    to_drop3 = list(dat2.filter(regex='Smoothed'))\n",
    "    dat3= dat2[dat2.columns.drop(to_drop3)]\n",
    "    #Remove Image_name\n",
    "    if ('image_name' in list(dat3.columns)):\n",
    "        dat =dat3.drop(columns=['Image_name'])\n",
    "    else:\n",
    "        dat=dat3\n",
    "    \n",
    "    cleaned_inputs_.append(dat)\n",
    "    print(\"Initial n_features:\",i.shape[1], \", After removal:\", dat.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the slides together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1031543, 51)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Class</th>\n",
       "      <th>Centroid_X</th>\n",
       "      <th>Centroid_Y</th>\n",
       "      <th>Detection probability</th>\n",
       "      <th>Nucleus: Area ¬µm^2</th>\n",
       "      <th>Nucleus: Length ¬µm</th>\n",
       "      <th>Nucleus: Circularity</th>\n",
       "      <th>Nucleus: Solidity</th>\n",
       "      <th>Nucleus: Max diameter ¬µm</th>\n",
       "      <th>...</th>\n",
       "      <th>NN_10_um</th>\n",
       "      <th>NN_20_um</th>\n",
       "      <th>NN_30_um</th>\n",
       "      <th>NN_40_um</th>\n",
       "      <th>NN_50_um</th>\n",
       "      <th>NN_60_um</th>\n",
       "      <th>NN_70_um</th>\n",
       "      <th>NN_80_um</th>\n",
       "      <th>NN_90_um</th>\n",
       "      <th>NN_100_um</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>721703.svs</td>\n",
       "      <td>Unlabelled</td>\n",
       "      <td>23317.2</td>\n",
       "      <td>303.63</td>\n",
       "      <td>0.8626</td>\n",
       "      <td>24.0212</td>\n",
       "      <td>17.8053</td>\n",
       "      <td>0.9522</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>6.4203</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>721703.svs</td>\n",
       "      <td>Unlabelled</td>\n",
       "      <td>23117.0</td>\n",
       "      <td>305.14</td>\n",
       "      <td>0.6539</td>\n",
       "      <td>28.8830</td>\n",
       "      <td>22.6611</td>\n",
       "      <td>0.7068</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>8.9267</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>721703.svs</td>\n",
       "      <td>Unlabelled</td>\n",
       "      <td>23301.3</td>\n",
       "      <td>310.54</td>\n",
       "      <td>0.8008</td>\n",
       "      <td>57.8022</td>\n",
       "      <td>28.1008</td>\n",
       "      <td>0.9199</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>10.7756</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>721703.svs</td>\n",
       "      <td>Unlabelled</td>\n",
       "      <td>23204.2</td>\n",
       "      <td>309.59</td>\n",
       "      <td>0.8541</td>\n",
       "      <td>29.3505</td>\n",
       "      <td>21.2158</td>\n",
       "      <td>0.8194</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>7.9087</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>721703.svs</td>\n",
       "      <td>Unlabelled</td>\n",
       "      <td>23098.8</td>\n",
       "      <td>310.98</td>\n",
       "      <td>0.7954</td>\n",
       "      <td>25.5138</td>\n",
       "      <td>19.5136</td>\n",
       "      <td>0.8420</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>7.4015</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Image       Class  Centroid_X  Centroid_Y  Detection probability  \\\n",
       "0  721703.svs  Unlabelled     23317.2      303.63                 0.8626   \n",
       "1  721703.svs  Unlabelled     23117.0      305.14                 0.6539   \n",
       "2  721703.svs  Unlabelled     23301.3      310.54                 0.8008   \n",
       "3  721703.svs  Unlabelled     23204.2      309.59                 0.8541   \n",
       "4  721703.svs  Unlabelled     23098.8      310.98                 0.7954   \n",
       "\n",
       "   Nucleus: Area ¬µm^2  Nucleus: Length ¬µm  Nucleus: Circularity  \\\n",
       "0              24.0212              17.8053                0.9522   \n",
       "1              28.8830              22.6611                0.7068   \n",
       "2              57.8022              28.1008                0.9199   \n",
       "3              29.3505              21.2158                0.8194   \n",
       "4              25.5138              19.5136                0.8420   \n",
       "\n",
       "   Nucleus: Solidity  Nucleus: Max diameter ¬µm  ...  NN_10_um  NN_20_um  \\\n",
       "0             1.0000                     6.4203  ...         0         1   \n",
       "1             0.9962                     8.9267  ...         0         2   \n",
       "2             1.0000                    10.7756  ...         0         1   \n",
       "3             0.9948                     7.9087  ...         0         0   \n",
       "4             1.0000                     7.4015  ...         0         3   \n",
       "\n",
       "   NN_30_um  NN_40_um  NN_50_um  NN_60_um  NN_70_um  NN_80_um  NN_90_um  \\\n",
       "0         2         3         3         8         8         9        12   \n",
       "1         4         6         9        12        15        18        25   \n",
       "2         2         5         8         9        10        14        17   \n",
       "3         2         5         7        10        13        21        30   \n",
       "4         5         6         8        10        14        19        24   \n",
       "\n",
       "   NN_100_um  \n",
       "0         17  \n",
       "1         30  \n",
       "2         20  \n",
       "3         34  \n",
       "4         31  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Variables: labelled_orig, labelled_data \n",
    "#1) Put the slides together\n",
    "\n",
    "labelled_orig = pd.concat(cleaned_inputs_)\n",
    "print(labelled_orig.shape)\n",
    "\n",
    "# 2) Extract relevant columns \n",
    "dat = labelled_orig.drop(columns=['Name','Parent','ROI']) \n",
    "dat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Image',\n",
       " 'Class',\n",
       " 'Centroid_X',\n",
       " 'Centroid_Y',\n",
       " 'Detection probability',\n",
       " 'Nucleus: Area ¬µm^2',\n",
       " 'Nucleus: Length ¬µm',\n",
       " 'Nucleus: Circularity',\n",
       " 'Nucleus: Solidity',\n",
       " 'Nucleus: Max diameter ¬µm',\n",
       " 'Nucleus: Min diameter ¬µm',\n",
       " 'Cell: Area ¬µm^2',\n",
       " 'Cell: Length ¬µm',\n",
       " 'Cell: Circularity',\n",
       " 'Cell: Solidity',\n",
       " 'Cell: Max diameter ¬µm',\n",
       " 'Cell: Min diameter ¬µm',\n",
       " 'Nucleus/Cell area ratio',\n",
       " 'Hematoxylin: Nucleus: Mean',\n",
       " 'Hematoxylin: Nucleus: Median',\n",
       " 'Hematoxylin: Nucleus: Min',\n",
       " 'Hematoxylin: Nucleus: Max',\n",
       " 'Hematoxylin: Nucleus: Std.Dev.',\n",
       " 'Hematoxylin: Cytoplasm: Mean',\n",
       " 'Hematoxylin: Cytoplasm: Median',\n",
       " 'Hematoxylin: Cytoplasm: Min',\n",
       " 'Hematoxylin: Cytoplasm: Max',\n",
       " 'Hematoxylin: Cytoplasm: Std.Dev.',\n",
       " 'Hematoxylin: Membrane: Mean',\n",
       " 'Hematoxylin: Membrane: Median',\n",
       " 'Hematoxylin: Membrane: Min',\n",
       " 'Hematoxylin: Membrane: Max',\n",
       " 'Hematoxylin: Membrane: Std.Dev.',\n",
       " 'Hematoxylin: Cell: Mean',\n",
       " 'Hematoxylin: Cell: Median',\n",
       " 'Hematoxylin: Cell: Min',\n",
       " 'Hematoxylin: Cell: Max',\n",
       " 'Hematoxylin: Cell: Std.Dev.',\n",
       " 'NN_10_um',\n",
       " 'NN_20_um',\n",
       " 'NN_30_um',\n",
       " 'NN_40_um',\n",
       " 'NN_50_um',\n",
       " 'NN_60_um',\n",
       " 'NN_70_um',\n",
       " 'NN_80_um',\n",
       " 'NN_90_um',\n",
       " 'NN_100_um']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dat.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting relevant cell classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1031543 cells\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unlabelled    1029835\n",
       "Neuron            476\n",
       "Oligo             445\n",
       "Ignore            244\n",
       "Astro             220\n",
       "Epithelial        159\n",
       "Endo              123\n",
       "Ambiguous          33\n",
       "Fragmented          5\n",
       "Region              2\n",
       "fragmented          1\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Check no. of cells / class of our data\n",
    "print(\"Total\",sum(dat['Class'].value_counts()),\"cells\")\n",
    "dat['Class'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note that we did not use 'ambiguous cells'***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Selecting only relevant cell classes (Using stardist_error instead of ignore_new)\n",
    "orig = dat.copy()\n",
    "dat__ = dat[(dat['Class'] == 'Oligo') | (dat['Class'] == 'Neuron')\n",
    "          | (dat['Class'] == 'Astro')| (dat['Class'] == 'Epithelial')\n",
    "          | (dat['Class'] == 'Ignore')| (dat['Class'] == 'Fragmented')| (dat['Class'] == 'Endo')]\n",
    "dat__=dat__.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Changing Epithelial to Endothelial\n",
    "classes = dat__['Class']\n",
    "formatted_classes = ['Endo'  if (i == 'Epithelial') else i for i in classes]\n",
    "dat_ = dat__.copy()\n",
    "dat_['Class']=formatted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neuron        476\n",
       "Oligo         445\n",
       "Endo          282\n",
       "Ignore        244\n",
       "Astro         220\n",
       "Fragmented      5\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Checking results from 3)\n",
    "dat_['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Others    531\n",
       "Neuron    476\n",
       "Oligo     445\n",
       "Astro     220\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) Group Ignore, Endo & Fragmented cells as a single class called 'Others'\n",
    "class_ = dat_['Class']\n",
    "y = ['Others'if i == 'Endo' or i == 'Ignore' or i == 'Fragmented' else i for i in class_ ]\n",
    "dat = dat_\n",
    "dat['Class'] = y \n",
    "print(dat['Class'].value_counts().sum())\n",
    "dat['Class'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1765\n",
    "Others    572\n",
    "Oligo     489\n",
    "Neuron    463\n",
    "Astro     241\n",
    "Name: Class, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Others    53.1\n",
       "Neuron    47.6\n",
       "Oligo     44.5\n",
       "Astro     22.0\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if cv=10\n",
    "dat['Class'].value_counts()/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for any NA in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any NA in the data?:  False\n"
     ]
    }
   ],
   "source": [
    "#checking for NAN \n",
    "## NEW \n",
    "print(\"Any NA in the data?: \",dat.isnull().sum().sum()==1)\n",
    "\n",
    "#dat = dat.dropna()\n",
    "# dat.isnull().sum().sum()\n",
    "#print(\"Any NA in the data?: \",dat.isnull().sum().sum()==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train, test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: (1672, 47)\n"
     ]
    }
   ],
   "source": [
    "#We are using the entire dataset to train the model, test data will be provided later by Sanne \n",
    "X_train_l = dat.drop(columns=['Class']) #,'Image_name','Image_x','Image_y'\n",
    "X_train = X_train_l.drop(columns=['Image','Centroid_X','Centroid_Y'])\n",
    "print('training data shape:',X_train_l.shape)\n",
    "y_train = dat['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Detection probability</th>\n",
       "      <th>Nucleus: Area ¬µm^2</th>\n",
       "      <th>Nucleus: Length ¬µm</th>\n",
       "      <th>Nucleus: Circularity</th>\n",
       "      <th>Nucleus: Solidity</th>\n",
       "      <th>Nucleus: Max diameter ¬µm</th>\n",
       "      <th>Nucleus: Min diameter ¬µm</th>\n",
       "      <th>Cell: Area ¬µm^2</th>\n",
       "      <th>Cell: Length ¬µm</th>\n",
       "      <th>Cell: Circularity</th>\n",
       "      <th>...</th>\n",
       "      <th>NN_10_um</th>\n",
       "      <th>NN_20_um</th>\n",
       "      <th>NN_30_um</th>\n",
       "      <th>NN_40_um</th>\n",
       "      <th>NN_50_um</th>\n",
       "      <th>NN_60_um</th>\n",
       "      <th>NN_70_um</th>\n",
       "      <th>NN_80_um</th>\n",
       "      <th>NN_90_um</th>\n",
       "      <th>NN_100_um</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8900</td>\n",
       "      <td>19.8128</td>\n",
       "      <td>15.8949</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>5.2491</td>\n",
       "      <td>4.8900</td>\n",
       "      <td>175.3026</td>\n",
       "      <td>47.1113</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7042</td>\n",
       "      <td>16.1802</td>\n",
       "      <td>16.5761</td>\n",
       "      <td>0.7400</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>6.9006</td>\n",
       "      <td>2.8992</td>\n",
       "      <td>129.4477</td>\n",
       "      <td>44.7169</td>\n",
       "      <td>0.8135</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9050</td>\n",
       "      <td>40.6065</td>\n",
       "      <td>23.0532</td>\n",
       "      <td>0.9602</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>8.2325</td>\n",
       "      <td>6.4008</td>\n",
       "      <td>170.7491</td>\n",
       "      <td>49.0544</td>\n",
       "      <td>0.8917</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8948</td>\n",
       "      <td>13.7914</td>\n",
       "      <td>13.2936</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>4.3656</td>\n",
       "      <td>4.0313</td>\n",
       "      <td>66.5980</td>\n",
       "      <td>30.8291</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8426</td>\n",
       "      <td>31.2085</td>\n",
       "      <td>20.1525</td>\n",
       "      <td>0.9657</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>7.0218</td>\n",
       "      <td>5.7947</td>\n",
       "      <td>191.4056</td>\n",
       "      <td>50.2637</td>\n",
       "      <td>0.9520</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Detection probability  Nucleus: Area ¬µm^2  Nucleus: Length ¬µm  \\\n",
       "0                 0.8900              19.8128              15.8949   \n",
       "1                 0.7042              16.1802              16.5761   \n",
       "2                 0.9050              40.6065              23.0532   \n",
       "3                 0.8948              13.7914              13.2936   \n",
       "4                 0.8426              31.2085              20.1525   \n",
       "\n",
       "   Nucleus: Circularity  Nucleus: Solidity  Nucleus: Max diameter ¬µm  \\\n",
       "0                0.9855             1.0000                     5.2491   \n",
       "1                0.7400             0.9797                     6.9006   \n",
       "2                0.9602             1.0000                     8.2325   \n",
       "3                0.9807             1.0000                     4.3656   \n",
       "4                0.9657             1.0000                     7.0218   \n",
       "\n",
       "   Nucleus: Min diameter ¬µm  Cell: Area ¬µm^2  Cell: Length ¬µm  \\\n",
       "0                     4.8900          175.3026           47.1113   \n",
       "1                     2.8992          129.4477           44.7169   \n",
       "2                     6.4008          170.7491           49.0544   \n",
       "3                     4.0313           66.5980           30.8291   \n",
       "4                     5.7947          191.4056           50.2637   \n",
       "\n",
       "   Cell: Circularity  ...  NN_10_um  NN_20_um  NN_30_um  NN_40_um  NN_50_um  \\\n",
       "0             0.9925  ...         0         0         0         1         5   \n",
       "1             0.8135  ...         0         0         0         2         3   \n",
       "2             0.8917  ...         0         2         2         3         5   \n",
       "3             0.8805  ...         1         2         3         4         4   \n",
       "4             0.9520  ...         0         1         1         4         4   \n",
       "\n",
       "   NN_60_um  NN_70_um  NN_80_um  NN_90_um  NN_100_um  \n",
       "0         8        12        16        22         23  \n",
       "1         6         8        10        17         21  \n",
       "2         5        12        15        17         20  \n",
       "3         7        11        15        17         21  \n",
       "4        11        12        15        18         23  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Detection probability', 'Nucleus: Area ¬µm^2', 'Nucleus: Length ¬µm',\n",
       "       'Nucleus: Circularity', 'Nucleus: Solidity',\n",
       "       'Nucleus: Max diameter ¬µm', 'Nucleus: Min diameter ¬µm',\n",
       "       'Cell: Area ¬µm^2', 'Cell: Length ¬µm', 'Cell: Circularity',\n",
       "       'Cell: Solidity', 'Cell: Max diameter ¬µm', 'Cell: Min diameter ¬µm',\n",
       "       'Nucleus/Cell area ratio', 'Hematoxylin: Nucleus: Mean',\n",
       "       'Hematoxylin: Nucleus: Median', 'Hematoxylin: Nucleus: Min',\n",
       "       'Hematoxylin: Nucleus: Max', 'Hematoxylin: Nucleus: Std.Dev.',\n",
       "       'Hematoxylin: Cytoplasm: Mean', 'Hematoxylin: Cytoplasm: Median',\n",
       "       'Hematoxylin: Cytoplasm: Min', 'Hematoxylin: Cytoplasm: Max',\n",
       "       'Hematoxylin: Cytoplasm: Std.Dev.', 'Hematoxylin: Membrane: Mean',\n",
       "       'Hematoxylin: Membrane: Median', 'Hematoxylin: Membrane: Min',\n",
       "       'Hematoxylin: Membrane: Max', 'Hematoxylin: Membrane: Std.Dev.',\n",
       "       'Hematoxylin: Cell: Mean', 'Hematoxylin: Cell: Median',\n",
       "       'Hematoxylin: Cell: Min', 'Hematoxylin: Cell: Max',\n",
       "       'Hematoxylin: Cell: Std.Dev.', 'NN_10_um', 'NN_20_um', 'NN_30_um',\n",
       "       'NN_40_um', 'NN_50_um', 'NN_60_um', 'NN_70_um', 'NN_80_um', 'NN_90_um',\n",
       "       'NN_100_um'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My own functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for custom classification metrics \n",
    "\n",
    "## Accuracy per class \n",
    "def astro_acc(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    acc_c = cm.diagonal()\n",
    "    return acc_c[0] #astrocytes acc\n",
    "\n",
    "def neuron_acc(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    acc_c = cm.diagonal()\n",
    "    return acc_c[1] #neuron acc\n",
    "\n",
    "def oligo_acc(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    acc_c = cm.diagonal()\n",
    "    return acc_c[2] #oligo acc\n",
    "\n",
    "def others_acc(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    acc_c = cm.diagonal()\n",
    "    return acc_c[3] #ignore acc\n",
    "\n",
    "\n",
    "\n",
    "## Confusion per class: \n",
    "\n",
    "## Astro\n",
    "def A_as_N(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    return cm[0][1] # percentage that A is wrongly classified as N   \n",
    "\n",
    "def A_as_O(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    return cm[0][2] # percentage that A is wrongly classified as O       \n",
    "\n",
    "\n",
    "def A_as_Others(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    return cm[0][3]   \n",
    "\n",
    "##Neurons \n",
    "\n",
    "def N_as_A(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    return cm[1][0] \n",
    "\n",
    "def N_as_O(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    return cm[1][2] \n",
    "\n",
    "def N_as_Others(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    return cm[1][3] \n",
    "\n",
    "\n",
    "## Oligo \n",
    "\n",
    "def O_as_A(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    return cm[2][0] \n",
    "\n",
    "def O_as_N(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    return cm[2][1] \n",
    "\n",
    "def O_as_Others(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    return cm[2][3] \n",
    "\n",
    "## Others \n",
    "\n",
    "def Others_as_A(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    return cm[3][0] \n",
    "\n",
    "def Others_as_N(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    return cm[3][1] \n",
    "\n",
    "def Others_as_O(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"],normalize='true')\n",
    "    return cm[3][2] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for custom classification metrics: RAW VALUES \n",
    "\n",
    "\n",
    "## Confusion per class: \n",
    "\n",
    "## Astro\n",
    "def A_as_N_r(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"])\n",
    "    return cm[0][1] # percentage that A is wrongly classified as N   \n",
    "\n",
    "def A_as_O_r(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"])\n",
    "    return cm[0][2] # percentage that A is wrongly classified as O       \n",
    "\n",
    "\n",
    "def A_as_Others_r(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"])\n",
    "    return cm[0][3]   \n",
    "\n",
    "##Neurons \n",
    "\n",
    "def N_as_A_r(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"])\n",
    "    return cm[1][0] \n",
    "\n",
    "def N_as_O_r(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"])\n",
    "    return cm[1][2] \n",
    "\n",
    "def N_as_Others_r(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"])\n",
    "    return cm[1][3] \n",
    "\n",
    "\n",
    "## Oligo \n",
    "\n",
    "def O_as_A_r(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"])\n",
    "    return cm[2][0] \n",
    "\n",
    "def O_as_N_r(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"])\n",
    "    return cm[2][1] \n",
    "\n",
    "def O_as_Others_r(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"])\n",
    "    return cm[2][3] \n",
    "\n",
    "## Others \n",
    "\n",
    "def Others_as_A_r(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"])\n",
    "    return cm[3][0] \n",
    "\n",
    "def Others_as_N_r(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"])\n",
    "    return cm[3][1] \n",
    "\n",
    "def Others_as_O_r(clf,X,y): \n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y,y_pred,labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"])\n",
    "    return cm[3][2] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Precision-recall score\n",
    "def precision_recall_auc(clf,X,y):\n",
    "    #Variables\n",
    "    pr_score={}\n",
    "    \n",
    "    #get y prob predictions\n",
    "    y_prob_pred = clf.predict_proba(X)\n",
    "    \n",
    "    #Convert true y name into numerical classes\n",
    "    y_true_numeric = name_to_numeric_classes(y)\n",
    "    \n",
    "    #get number of classes\n",
    "    n_class = list(set(y))\n",
    "    \n",
    "    #create PR curve using OVR approach \n",
    "    for i in range(len(n_class)): # for each class, calculate roc_curve \n",
    "        p, r, thresh = precision_recall_curve(y_true_numeric, y_prob_pred[:,i], pos_label=i)\n",
    "        pr_score[i] = auc(r,p) #recall on x axis, precision on y axis\n",
    "        \n",
    "    #Combine all pr-scores using 'macro' method\n",
    "    pr_auc = mean(pr_score.values())\n",
    "    return pr_auc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDITIONAL FUNCTIONS\n",
    "\n",
    "#FUNCTIONS\n",
    "\n",
    "# Get all class-specific thresholds from best params\n",
    "\n",
    "def get_threshold(best_params):\n",
    "    class_thresholds=[]\n",
    "    for i in best_params:\n",
    "        t = best_params[i][0]\n",
    "        class_thresholds.append(t)\n",
    "    return class_thresholds \n",
    "\n",
    "#Thresholding method 5: using ratio, ambiguous cells are: \n",
    "# 1) When predicted probabilities < thresholds, i.e. diff = -ve (lower end)\n",
    "# 2) When more than 1 class-specific threshold is passed, i.e. more than 1 positive diff scores (upper end)\n",
    "\n",
    "def threshold_list_of_classes_5(y_pred_prob,best_params):\n",
    "    \n",
    "    thresholded_classes=[]\n",
    "    for i in y_pred_prob: #for each cell (containing 4 class probabilities)\n",
    "        \n",
    "        #Get cell 4 class-specific threshold values: \n",
    "        thresholds = get_threshold(best_params)\n",
    "        \n",
    "        # Calculate predicted probability - threshold = difference for each of the 4 classes \n",
    "        differences = (i-thresholds)/thresholds\n",
    "        \n",
    "        #Count number of positive or equal (0) differences \n",
    "        count = np.count_nonzero(differences>=0)\n",
    "        \n",
    "        if (count==1): #only assign class when 1 class passes the threshold \n",
    "            pred_class = np.argmax(differences)\n",
    "        else: #Otherwise, label as ambiguous (when more than 1 class passes, or when no class passes)\n",
    "            pred_class=4\n",
    "\n",
    "        #putting prediction in a list\n",
    "        thresholded_classes.append(pred_class)\n",
    "\n",
    "    thresholded_classes_ = numeric_to_name_classes(thresholded_classes)\n",
    "    return thresholded_classes_\n",
    "             \n",
    "#Thresholding method 4  using ratio instead of raw difference\n",
    "def threshold_list_of_classes_4 (y_pred_prob,best_params):\n",
    "    \n",
    "    thresholded_classes=[]\n",
    "    for i in y_pred_prob: #for each cell (containing 4 class probabilities)\n",
    "        \n",
    "        #Get cell 4 class-specific threshold values: \n",
    "        thresholds = get_threshold(best_params)\n",
    "        \n",
    "        # Calculate predicted probability - threshold = difference for each of the 4 classes \n",
    "        differences = (i-thresholds)/thresholds\n",
    "        \n",
    "        #Check if there is at last one positive diff probability\n",
    "        if (np.any(differences>0)): # If true, \n",
    "            \n",
    "            #get index (indicative of class) of the highest probability difference\n",
    "            pred_class = np.argmax(differences) \n",
    "            \n",
    "        else: #If all diff probabilities are NEGATIVE\n",
    "            pred_class = 4 # Assign cell class as 'ambiguous'\n",
    "\n",
    "        #putting prediction in a list\n",
    "        thresholded_classes.append(pred_class)\n",
    "\n",
    "    thresholded_classes_ = numeric_to_name_classes(thresholded_classes)\n",
    "    return thresholded_classes_\n",
    "\n",
    "\n",
    "#Thresholding method 3)\n",
    "\n",
    "def threshold_list_of_classes_3 (y_pred_prob,best_params):\n",
    "    \n",
    "    thresholded_classes=[]\n",
    "    for i in y_pred_prob: #for each cell (containing 4 class probabilities)\n",
    "        \n",
    "        #Get cell 4 class-specific threshold values: \n",
    "        thresholds = get_threshold(best_params)\n",
    "        \n",
    "        # Calculate predicted probability - threshold = difference for each of the 4 classes \n",
    "        differences = i-thresholds\n",
    "        \n",
    "        #Check if there is at last one positive diff probability\n",
    "        if (np.any(differences>0)): # If true, \n",
    "            \n",
    "            #get index (indicative of class) of the highest probability difference\n",
    "            pred_class = np.argmax(differences) \n",
    "            \n",
    "        else: #If all diff probabilities are NEGATIVE\n",
    "            pred_class = 4 # Assign cell class as 'ambiguous'\n",
    "\n",
    "        #putting prediction in a list\n",
    "        thresholded_classes.append(pred_class)\n",
    "\n",
    "    thresholded_classes_ = numeric_to_name_classes(thresholded_classes)\n",
    "    return thresholded_classes_\n",
    "\n",
    "#Thresholding method 2)\n",
    "def threshold_list_of_classes_2(y_pred_prob,best_params):\n",
    "    \n",
    "    thresholded_classes=[]\n",
    "    for i in y_pred_prob: #for each cell \n",
    "        \n",
    "        #Get threshold values: \n",
    "        thresholds = get_threshold(best_params)\n",
    "        \n",
    "        # Calculate predicted probability - threshold = difference \n",
    "        differences = i-thresholds\n",
    "               \n",
    "        #get index (indicative of class) of the highest probability\n",
    "        pred_class = np.argmax(differences)         \n",
    "\n",
    "        #putting prediction in a list\n",
    "        thresholded_classes.append(pred_class)\n",
    "\n",
    "        \n",
    "    thresholded_classes_ = numeric_to_name_classes(thresholded_classes)\n",
    "    return thresholded_classes_\n",
    "\n",
    "\n",
    "# To convert numeric classes to its corresponding name classes \n",
    "def numeric_to_name_classes(numeric_classes):\n",
    "    output=[]\n",
    "    for i in numeric_classes:\n",
    "        if (i==0):\n",
    "            c = 'Astro'\n",
    "        elif(i==1):\n",
    "            c='Neuron'\n",
    "        elif(i==2):\n",
    "            c='Oligo'\n",
    "        elif(i==3):\n",
    "            c='Others'\n",
    "        elif(i==4):\n",
    "            c='Ambiguous'\n",
    "        else:\n",
    "            print('SOMETHING IS WRONG')\n",
    "        output.append(c)\n",
    "    return output\n",
    "\n",
    "# To convert name classes to its corresponding numeric classes: \n",
    "def name_to_numeric_classes(name_classes):\n",
    "    output=[]\n",
    "    for i in name_classes: \n",
    "        if (i == 'Astro'): \n",
    "            x=0\n",
    "        elif(i == 'Neuron'): \n",
    "            x=1\n",
    "        elif(i == 'Oligo'): \n",
    "            x=2\n",
    "        elif(i=='Others'):\n",
    "            x=3\n",
    "        elif(i=='Ambiguous'):\n",
    "            x=4\n",
    "        else:\n",
    "            print('SOMETHING IS WRONG')\n",
    "            break\n",
    "        output.append(x)\n",
    "    return output\n",
    "    \n",
    "# Create roc curve for each class in multi-classification problem\n",
    "\n",
    "def multiclass_roc_curves(n_class,test_y_numeric,predy):\n",
    "    \n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    thresh ={}\n",
    "    \n",
    "    #calcualte roc curve locations \n",
    "    for i in range(n_class): # for each class, calculate roc_curve \n",
    "        fpr[i], tpr[i], thresh[i] = roc_curve(test_y_numeric, predy[:,i], pos_label=i)\n",
    "    \n",
    "    return fpr,tpr,thresh\n",
    "\n",
    "#Find the best position on the roc curve for multi-classification problem\n",
    "def best_param_gmean(n_class,fpr,tpr,thresh):\n",
    "    #calculate g-mean for each threshold \n",
    "    #gmeans={}\n",
    "    best_params={}\n",
    "    class_names=['Astro','Neuron','Oligo','Others']\n",
    "    for i in range(n_class):\n",
    "        tpr_ = tpr[i]\n",
    "        fpr_ = fpr[i]\n",
    "        gm = np.sqrt(tpr_*(1-fpr_))\n",
    "        t = thresh[i]\n",
    "        #gmeans[i]=gm\n",
    "        ix = np.argmax(gm)\n",
    "        best_params[i] = (t[ix],gm[ix],fpr_[ix],tpr_[ix])\n",
    "        #print(gm)\n",
    "       # print(class_names[i],'Best Threshold=%f, G-Mean=%.3f, fpr=%f, tpr=%f' % (t[ix], gm[ix],fpr_[ix],tpr_[ix]))\n",
    "        #print('--------------------------------------------------')\n",
    "    return best_params\n",
    "\n",
    "def multiclass_PR_curves(n_class,test_y_numeric,predy):\n",
    "    \n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    thresh ={}\n",
    "    \n",
    "    #calcualte roc curve locations \n",
    "    for i in range(n_class): # for each class, calculate roc_curve \n",
    "        precision[i], recall[i], thresh[i] = precision_recall_curve(test_y_numeric, predy[:,i], pos_label=i)\n",
    "    \n",
    "    return precision,recall,thresh\n",
    "\n",
    "#Find the best position on the roc curve for multi-classification problem\n",
    "def best_param_f_score(n_class,precision,recall,thresh):\n",
    "    #calculate g-mean for each threshold \n",
    "    #f_scores={}\n",
    "    best_params={}\n",
    "    class_names=['Astro','Neuron','Oligo','Others']\n",
    "    for i in range(n_class):\n",
    "        p = precision[i]\n",
    "        r = recall[i]\n",
    "        nu=(2*p*r)\n",
    "        de=(p+r) \n",
    "        f_score = np.divide(nu,de,out=np.zeros_like(nu),where=de != 0) #(2*p*r)/(p+r)\n",
    "        t = thresh[i]\n",
    "        #f_scores[i]=f_score\n",
    "        ix = np.argmax(f_score)\n",
    "        best_params[i] = (t[ix],f_score[ix],p[ix],r[ix])\n",
    "        #print(gm)\n",
    "       # print(class_names[i],'Best Threshold=%f, G-Mean=%.3f, fpr=%f, tpr=%f' % (t[ix], gm[ix],fpr_[ix],tpr_[ix]))\n",
    "        #print('--------------------------------------------------')\n",
    "    return best_params\n",
    "\n",
    "#Thresholding function\n",
    "def prob_thresholding(y_pred_prob,y_pred,threshold):\n",
    "    thresholded_class =[]\n",
    "    for i in range(0,len(y_pred_prob)):\n",
    "        if(max(y_pred_prob[i])<threshold):\n",
    "            c='Ambiguous'\n",
    "        else:\n",
    "            c=y_pred[i]\n",
    "        thresholded_class.append(c)\n",
    "    return thresholded_class\n",
    "\n",
    "#Removing ambiguous class from thresholded class & y_predict\n",
    "def remove_amb_class(t_class,y_test):\n",
    "    \n",
    "    #Get indices of instances with no ambiguous label \n",
    "    x = pd.Series(t_class)\n",
    "    y_pred_no_amb = x[x!='Ambiguous']\n",
    "    y_pred_no_amb_indices = y_pred_no_amb.index\n",
    "    \n",
    "    #Extract these instances fom y_pred\n",
    "    #y_predict_no_amb = y_predict.iloc[pred_no_amb_indices]\n",
    "    \n",
    "    #Subset y_test\n",
    "    y_test_no_amb = y_test.iloc[y_pred_no_amb_indices]\n",
    "    \n",
    "    return (y_pred_no_amb,y_test_no_amb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning - random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('normalizer', MinMaxScaler()),\n",
       " ('selector', RFE(estimator=SVC(kernel='linear'))),\n",
       " ('clf', BalancedRandomForestClassifier())]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('normalizer',MinMaxScaler()),\n",
    "    ('selector',RFE(SVC(kernel='linear'))), \n",
    "    ('clf',BalancedRandomForestClassifier())\n",
    "])\n",
    "#pipeline.set_params(clf=RandomForestClassifier())\n",
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccp_alphas = [float(x) for x in np.linspace(start=0, stop=0.03, num=7) ]\n",
    "ccp_alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "0.8226719689223525\n",
      "{'selector__n_features_to_select': 32, 'clf__sampling_strategy': 'not majority', 'clf__random_state': 42, 'clf__n_estimators': 1800, 'clf__min_samples_split': 2, 'clf__min_samples_leaf': 2, 'clf__max_features': 'auto', 'clf__max_depth': 40, 'clf__ccp_alpha': 0.005, 'clf__bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "### Hyper parameters to tune\n",
    "\n",
    "#Number of trees in random forest \n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "#Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "#Maximum number of levels in tree \n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "#Minimum number of samples required to split an internal node \n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "#Minimum number of samples required at each leaf node \n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "#Method for selecting samples for training each tree \n",
    "bootstrap = [True, False]\n",
    "\n",
    "#sampling strategy\n",
    "sampling_strategy=['auto','all','not majority','majority']\n",
    "\n",
    "#ccp_alphas \n",
    "ccp_alphas = [float(x) for x in np.linspace(start=0, stop=0.03, num=7) ]#[float(x) for x in np.linspace(start=0, stop=1, num=10) ]\n",
    "\n",
    "## Create the random grid \n",
    "random_grid = {'selector__n_features_to_select':[30,32,34,36,38,40],\n",
    "                'clf__n_estimators': n_estimators,\n",
    "               'clf__max_features': max_features,\n",
    "               'clf__max_depth': max_depth,\n",
    "               'clf__min_samples_split': min_samples_split,\n",
    "               'clf__min_samples_leaf': min_samples_leaf,\n",
    "              'clf__bootstrap': bootstrap,\n",
    "              'clf__random_state':[42],\n",
    "               'clf__sampling_strategy':sampling_strategy, \n",
    "               'clf__ccp_alpha':ccp_alphas\n",
    "             # 'clf__class_weight':['balanced']\n",
    "              } # newly added\n",
    "#pprint(random_grid)\n",
    "\n",
    "rf_random = RandomizedSearchCV(pipeline,\n",
    "                             param_distributions=random_grid, \n",
    "                             n_iter=100,\n",
    "                             cv=10,\n",
    "                             verbose=2,\n",
    "                            random_state=42,\n",
    "                            n_jobs=-1,\n",
    "                              refit='PR_AUC', # use this metric to evaluate performance of parameters \n",
    "                      scoring={'PR_AUC':precision_recall_auc,\n",
    "                          'roc_auc_ovr_weighted':'roc_auc_ovr_weighted',\n",
    "                            'roc_auc_ovo':'roc_auc_ovo',\n",
    "                              'balanced_accuracy':'balanced_accuracy',\n",
    "                               'f1_weighted':'f1_weighted',\n",
    "                               'Astro_accuracy': astro_acc,\n",
    "                               'Neuron_accuracy':neuron_acc,\n",
    "                               'Oligo_accuracy':oligo_acc,\n",
    "                               'Others_accuracy':others_acc,\n",
    "                               'A_as_N':A_as_N,\n",
    "                               'A_as_O':A_as_O,\n",
    "                               'A_as_Others':A_as_Others,\n",
    "                               'N_as_A':N_as_A,\n",
    "                               'N_as_O':N_as_O,\n",
    "                               'N_as_Others':N_as_Others,\n",
    "                               'O_as_A':O_as_A,\n",
    "                               'O_as_N':O_as_N,\n",
    "                               'O_as_Others':O_as_Others,\n",
    "                               'Others_as_A':Others_as_A,\n",
    "                               'Others_as_N':Others_as_N,\n",
    "                               'Others_as_O':Others_as_O\n",
    "                              })\n",
    "\n",
    "rf_random.fit(X_train,y_train)\n",
    "\n",
    "print(rf_random.best_score_)\n",
    "print(rf_random.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
    "0.8226719689223525\n",
    "{'selector__n_features_to_select': 32, 'clf__sampling_strategy': 'not majority', 'clf__random_state': 42, 'clf__n_estimators': 1800, 'clf__min_samples_split': 2, 'clf__min_samples_leaf': 2, 'clf__max_features': 'auto', 'clf__max_depth': 40, 'clf__ccp_alpha': 0.005, 'clf__bootstrap': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR-AUC: 82.26719689223525\n",
      "ROC-AUC: 94.54720213043537\n",
      "ROC-AUC: 93.68336041410387\n",
      "Balanced accuracy: 73.64612432196988\n",
      "F1_weighted: 75.44660728939479\n",
      "Astrocyte accuracy: 65.45454545454545\n",
      "Neuron accuracy: 74.59663120567376\n",
      "Oligo accuracy: 69.4040404040404\n",
      "Others accuracy: 85.12928022361986\n",
      "Classified A as N: 14.545454545454545\n",
      "Classified A as O: 6.8181818181818175\n",
      "Classified A as Others: 13.18181818181818\n",
      "Classified N as A: 13.834219858156027\n",
      "Classified N as O: 0.42109929078014185\n",
      "Classified N as Others: 11.14804964539007\n",
      "Classified O as A: 11.712121212121211\n",
      "Classified O as N: 0.0\n",
      "Classified O as Others: 18.883838383838388\n",
      "Classified Others as A: 3.7700908455625433\n",
      "Classified Others as N: 5.066387141858839\n",
      "Classified Others as O: 6.03424178895877\n"
     ]
    }
   ],
   "source": [
    "# Digging into more details \n",
    "print(\"PR-AUC:\",\n",
    "     rf_random.cv_results_['mean_test_PR_AUC'][rf_random.best_index_]*100)\n",
    "print(\"ROC-AUC:\",\n",
    "     rf_random.cv_results_['mean_test_roc_auc_ovr_weighted'][rf_random.best_index_]*100)\n",
    "print(\"ROC-AUC:\",\n",
    "     rf_random.cv_results_['mean_test_roc_auc_ovo'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Balanced accuracy:\",\n",
    "      rf_random.cv_results_['mean_test_balanced_accuracy'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"F1_weighted:\",\n",
    "      rf_random.cv_results_['mean_test_f1_weighted'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Astrocyte accuracy:\",\n",
    "      rf_random.cv_results_['mean_test_Astro_accuracy'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Neuron accuracy:\",\n",
    "      rf_random.cv_results_['mean_test_Neuron_accuracy'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Oligo accuracy:\",\n",
    "      rf_random.cv_results_['mean_test_Oligo_accuracy'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Others accuracy:\",\n",
    "      rf_random.cv_results_['mean_test_Others_accuracy'][rf_random.best_index_]*100)\n",
    "\n",
    "\n",
    "print(\"Classified A as N:\",\n",
    "      rf_random.cv_results_['mean_test_A_as_N'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Classified A as O:\",\n",
    "      rf_random.cv_results_['mean_test_A_as_O'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Classified A as Others:\",\n",
    "      rf_random.cv_results_['mean_test_A_as_Others'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Classified N as A:\",\n",
    "      rf_random.cv_results_['mean_test_N_as_A'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Classified N as O:\",\n",
    "      rf_random.cv_results_['mean_test_N_as_O'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Classified N as Others:\",\n",
    "      rf_random.cv_results_['mean_test_N_as_Others'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Classified O as A:\",\n",
    "      rf_random.cv_results_['mean_test_O_as_A'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Classified O as N:\",\n",
    "      rf_random.cv_results_['mean_test_O_as_N'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Classified O as Others:\",\n",
    "      rf_random.cv_results_['mean_test_O_as_Others'][rf_random.best_index_]*100)\n",
    "\n",
    "\n",
    "print(\"Classified Others as A:\",\n",
    "      rf_random.cv_results_['mean_test_Others_as_A'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Classified Others as N:\",\n",
    "      rf_random.cv_results_['mean_test_Others_as_N'][rf_random.best_index_]*100)\n",
    "\n",
    "print(\"Classified Others as O:\",\n",
    "      rf_random.cv_results_['mean_test_Others_as_O'][rf_random.best_index_]*100)\n",
    "                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PR-AUC: 82.26719689223525\n",
    "ROC-AUC: 94.54720213043537\n",
    "ROC-AUC: 93.68336041410387\n",
    "Balanced accuracy: 73.64612432196988\n",
    "F1_weighted: 75.44660728939479\n",
    "Astrocyte accuracy: 65.45454545454545\n",
    "Neuron accuracy: 74.59663120567376\n",
    "Oligo accuracy: 69.4040404040404\n",
    "Others accuracy: 85.12928022361986\n",
    "Classified A as N: 14.545454545454545\n",
    "Classified A as O: 6.8181818181818175\n",
    "Classified A as Others: 13.18181818181818\n",
    "Classified N as A: 13.834219858156027\n",
    "Classified N as O: 0.42109929078014185\n",
    "Classified N as Others: 11.14804964539007\n",
    "Classified O as A: 11.712121212121211\n",
    "Classified O as N: 0.0\n",
    "Classified O as Others: 18.883838383838388\n",
    "Classified Others as A: 3.7700908455625433\n",
    "Classified Others as N: 5.066387141858839\n",
    "Classified Others as O: 6.03424178895877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual cross validation, using PR curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=None, shuffle=False)\n",
      "mean ROC AUC: 0.9454720213043535\n",
      "--------------------------------\n",
      "mean log loss: 0.6257403525336385\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "### manual cross-validation method \n",
    "x= X_train\n",
    "y=y_train\n",
    "## Setting up cross-validation \n",
    "skf = StratifiedKFold(n_splits=10) # shuffling = False, no need to set random_state\n",
    "skf.get_n_splits(x,y) # using only training data\n",
    "print(skf)\n",
    "\n",
    "#for train_index, test_index in skf.split(x,y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    #print(\"Train_size:\", train_index.size, \"Test_size:\", test_index.size)\n",
    "    \n",
    "## Training \n",
    "accuracies = []\n",
    "accuracies_c = []\n",
    "\n",
    "t_accuracies= []\n",
    "t_accuracies_c= []\n",
    "\n",
    "reports = []\n",
    "reports_c = []\n",
    "\n",
    "t_reports= []\n",
    "t_reports_c= []\n",
    "\n",
    "confusion_matrices = []\n",
    "confusion_matrices_c = [] \n",
    "\n",
    "t_confusion_matrices=[]\n",
    "t_confusion_matrices_c=[]\n",
    "\n",
    "#train_features =[] \n",
    "#train_n_features=[]\n",
    "y_preds = []\n",
    "y_preds_c = []\n",
    "\n",
    "y_preds_t =[]\n",
    "y_preds_t_c =[]\n",
    "\n",
    "y_prob_preds = []\n",
    "y_prob_preds_c = []\n",
    "\n",
    "y_cv_test=[]\n",
    "x_cv_test=[]\n",
    "\n",
    "roc_auc_scores=[]\n",
    "roc_auc_scores_c=[]\n",
    "\n",
    "log_losses=[]\n",
    "log_losses_c=[]\n",
    "\n",
    "#brier_scores=[]\n",
    "#brier_scores_c=[]\n",
    "\n",
    "best_parameters=[]\n",
    "best_parameters_c=[]\n",
    "\n",
    "for train_index, test_index in skf.split(x,y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    #print(\"Train_size:\", train_index.size, \"Test_size:\", test_index.size)\n",
    "    x_train_, x_test_ = x.iloc[train_index], x.iloc[test_index]\n",
    "    y_train_, y_test_ = y[train_index], y[test_index]\n",
    "    x_test_l = X_train_l.iloc[test_index]\n",
    "    #print(\"n of each cell types at test:\\n\", y_test_.value_counts())\n",
    "    \n",
    "    ## 1) Create classifier \n",
    "    \n",
    "    model=Pipeline([\n",
    "    ('normalizer',MinMaxScaler()),\n",
    "    ('selector',RFE(SVC(kernel='linear'),n_features_to_select=32)), \n",
    "    ('clf', BalancedRandomForestClassifier(sampling_strategy= 'not majority', random_state= 42,\n",
    "                                           n_estimators= 1800, min_samples_split= 2, min_samples_leaf= 2,\n",
    "                                           max_features= 'auto', max_depth= 40, ccp_alpha= 0.005, bootstrap= True))\n",
    "                                           ### MAKE SURE THESE ARE CORRECT \n",
    "                \n",
    "])\n",
    "    \n",
    "# {'selector__n_features_to_select': 32, 'clf__sampling_strategy': 'not majority', 'clf__random_state': 42,\n",
    "#  'clf__n_estimators': 1800, 'clf__min_samples_split': 2, 'clf__min_samples_leaf': 2,\n",
    "#  'clf__max_features': 'auto', 'clf__max_depth': 40, 'clf__ccp_alpha': 0.005, 'clf__bootstrap': True}\n",
    "\n",
    "    # 2) Train the calibrated classifier with 'training' data (x,y) \n",
    "    model.fit(x_train_,y_train_)\n",
    "    \n",
    "    # 3) Get class probability predictions for 'test' data \n",
    "    y_prob_predict = model.predict_proba(x_test_)\n",
    "    \n",
    "    # 4.1) For thresholding: convert y_test_ from name classes to numeric classes\n",
    "    y_test_numeric = name_to_numeric_classes(y_test_)\n",
    "    \n",
    "    # 4.2) For thresholding: use predicted class probabilities to calculate ROC curve for each class vs rest\n",
    "    \n",
    "    precision,recall,thresh = multiclass_PR_curves(4,y_test_numeric,y_prob_predict)\n",
    "    \n",
    "    # 4.3) For thresholding: from ROC curves, find the best location (fpr,tpr,thresh) for each class \n",
    "    # Evaluated based on g-mean \n",
    "    best_params_ = best_param_f_score(4,precision,recall,thresh)\n",
    "    best_parameters.append(best_params_)\n",
    "    \n",
    "    # 4.4) For thresholding: apply thresholding to each class to create crisp class label \n",
    "    t_class = threshold_list_of_classes_5(y_prob_predict,best_params_)\n",
    "    \n",
    "    # 5) Get class labels using default thresholding value (0.5)\n",
    "    y_predict = model.predict(x_test_)\n",
    "    \n",
    "    # 6) Put predictions (labels &probabilities & t_labels) in the corresponding list\n",
    "    y_preds.append(y_predict)\n",
    "    \n",
    "    y_prob_preds.append(y_prob_predict)\n",
    "    \n",
    "    y_preds_t.append(t_class)\n",
    "    \n",
    "    y_cv_test.append(y_test_) # for visualisation purposes later on\n",
    "    x_cv_test.append(x_test_l)\n",
    "    \n",
    "    # 7) Remove 'ambiguous class' from t_class & y_test_ - for accuracy calculation\n",
    "    (y_predict_no_amb,y_test_no_amb) = remove_amb_class(t_class,y_test_)\n",
    "    \n",
    "    # 8) Calculate and put Performance metric (balanced accuracy) per fold into a list\n",
    "    accuracies.append(balanced_accuracy_score(y_test_,y_predict)) ## using BALANCED ACC.\n",
    "    \n",
    "    t_accuracies.append(balanced_accuracy_score(y_test_no_amb,y_predict_no_amb))\n",
    "    \n",
    "    #8.1) Compute classification reports\n",
    "    reports.append(classification_report(y_test_,y_predict,output_dict=True)) \n",
    "  #  reports_c.append(classification_report(y_test_,y_predict_c,output_dict=True)) \n",
    "    \n",
    "    t_reports.append(classification_report(y_test_no_amb,y_predict_no_amb,output_dict=True))\n",
    "    \n",
    "    # 9) Calculate and put ROC AUC scores per fold into a list \n",
    "    roc_auc_scores.append(roc_auc_score(y_test_,y_prob_predict,multi_class='ovr',average='weighted'))\n",
    "    \n",
    "    #9.1) Calculate and put log loss per fold into a list\n",
    "    log_losses.append(log_loss(y_test_,y_prob_predict))\n",
    "\n",
    "    \n",
    "    # 10) Create confusion matrices for default & thresholded results per fold then put in a list \n",
    "    cm = confusion_matrix(y_test_,y_predict, labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"]) #,normalize='true'\n",
    "    \n",
    "    cm_t = confusion_matrix(y_test_no_amb,y_predict_no_amb, labels=[\"Astro\",\"Neuron\",\"Oligo\",\"Others\"])#,normalize='true'\n",
    "    \n",
    "    confusion_matrices.append(cm)\n",
    "    \n",
    "    t_confusion_matrices.append(cm_t)\n",
    "\n",
    "print('mean ROC AUC:',mean(roc_auc_scores))\n",
    "print('--------------------------------')\n",
    "print('mean log loss:',mean(log_losses))\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting information from best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thresholds \n",
    "astro_t = [] \n",
    "neuron_t=[]\n",
    "oligo_t=[]\n",
    "others_t=[]\n",
    "#G-means\n",
    "astro_gm=[]\n",
    "neuron_gm=[]\n",
    "oligo_gm=[]\n",
    "others_gm=[]\n",
    "\n",
    "#Info extraction \n",
    "for fold in best_parameters:\n",
    "    a_t = fold[0][0]\n",
    "    n_t = fold[1][0]\n",
    "    o_t = fold[2][0]\n",
    "    ot_t = fold[3][0]\n",
    "    \n",
    "    a_gm = fold[0][1]\n",
    "    n_gm = fold[1][1]\n",
    "    o_gm = fold[2][1]\n",
    "    ot_gm = fold[3][1]\n",
    "    \n",
    "    astro_t.append(a_t)\n",
    "    neuron_t.append(n_t)\n",
    "    oligo_t.append(o_t)\n",
    "    others_t.append(ot_t)\n",
    "    \n",
    "    astro_gm.append(a_gm)\n",
    "    neuron_gm.append(n_gm)\n",
    "    oligo_gm.append(o_gm)\n",
    "    others_gm.append(ot_gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NON CALIBRATED\n",
      "mean astro threshold: 0.35793807456439586 , mean f1_macro: 0.6340967926090835\n",
      "mean neuron threshold: 0.2695591565163307 , mean f1_macro: 0.8470746099012754\n",
      "mean oligo threshold: 0.2639737969421004 , mean f1_macro: 0.8587408110939749\n",
      "mean others threshold: 0.5144890330406827 , mean f1_macro: 0.8309850166507259\n",
      "1.4059600610635097\n"
     ]
    }
   ],
   "source": [
    "print(\"NON CALIBRATED\")\n",
    "print('mean astro threshold:', mean(astro_t), ', mean f1_macro:',mean(astro_gm))\n",
    "print('mean neuron threshold:', mean(neuron_t), ', mean f1_macro:',mean(neuron_gm))\n",
    "print('mean oligo threshold:', mean(oligo_t), ', mean f1_macro:',mean(oligo_gm))\n",
    "print('mean others threshold:', mean(others_t), ', mean f1_macro:',mean(others_gm))\n",
    "print(mean(astro_t)+mean(neuron_t)+mean(oligo_t)+mean(others_t)) #is it okay that it is above 1? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NO Thresholding:** Non-calibrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with no thresholding: 73.64612432196986\n",
      "Macro avg F1  72.97174847999646\n",
      "Weighted avg F1  75.44660728939479\n",
      "--------------------------\n",
      "[[144  32  15  29]\n",
      " [ 66 355   2  53]\n",
      " [ 52   0 309  84]\n",
      " [ 20  27  32 452]]\n",
      "[[65.45454545 14.54545455  6.81818182 13.18181818]\n",
      " [13.86554622 74.57983193  0.42016807 11.13445378]\n",
      " [11.68539326  0.         69.43820225 18.87640449]\n",
      " [ 3.76647834  5.08474576  6.02636535 85.12241055]]\n",
      "--------------------------\n",
      "Astro accuracy 65.45454545454545\n",
      "Neuron accuracy 74.57983193277312\n",
      "Oligo accuracy 69.43820224719101\n",
      "Others accuracy 85.12241054613936\n",
      "--------------------------\n",
      "Astro f1-score  57.78839338458975\n",
      "Neuron f1-score  79.43690440854768\n",
      "Oligo f1-score  75.64718874858124\n",
      "Others f1-score  79.01450737826718\n",
      "--------------------------\n",
      "Macro avg precision 74.86092179569212\n",
      "Macro avg recall  73.64612432196986\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix across 10 folds, WITHOUT thresholding \n",
    "print('with no thresholding:',mean(accuracies)*100)\n",
    "print('Macro avg F1 ',mean([i['macro avg']['f1-score'] for i in reports])*100)\n",
    "print('Weighted avg F1 ',mean([i['weighted avg']['f1-score'] for i in reports])*100)\n",
    "\n",
    "print(\"--------------------------\")\n",
    "C=sum(confusion_matrices)\n",
    "final_cm =  C.astype('float') / C.sum(axis=1)[:, np.newaxis]*100 #normalize(sum(confusion_matrices))*100\n",
    "print(C)\n",
    "print(final_cm)\n",
    "print(\"--------------------------\")\n",
    "print(\"Astro accuracy\",final_cm[0][0])\n",
    "print(\"Neuron accuracy\",final_cm[1][1])\n",
    "print(\"Oligo accuracy\",final_cm[2][2])\n",
    "print(\"Others accuracy\",final_cm[3][3])\n",
    "print(\"--------------------------\")\n",
    "# F1-score per class: \n",
    "print('Astro f1-score ',mean([i['Astro']['f1-score'] for i in reports])*100)\n",
    "print('Neuron f1-score ',mean([i['Neuron']['f1-score'] for i in reports])*100)\n",
    "print('Oligo f1-score ',mean([i['Oligo']['f1-score'] for i in reports])*100)\n",
    "print('Others f1-score ',mean([i['Others']['f1-score'] for i in reports])*100)\n",
    "print(\"--------------------------\")\n",
    "print('Macro avg precision',mean([i['macro avg']['precision'] for i in reports])*100)\n",
    "print('Macro avg recall ',mean([i['macro avg']['recall'] for i in reports])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thresholding:** Non-calibrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with thresholding (non-calibrated) ACC : 85.02476036879838\n",
      "Macro avg F1  84.60261078116721\n",
      "Weighted avg F1  87.00641848988474\n",
      "--------------------------\n",
      "[[107  22   8   9]\n",
      " [ 15 363   3  18]\n",
      " [ 15   0 341  22]\n",
      " [ 13  26  29 397]]\n",
      "[[73.28767123 15.06849315  5.47945205  6.16438356]\n",
      " [ 3.7593985  90.97744361  0.7518797   4.5112782 ]\n",
      " [ 3.96825397  0.         90.21164021  5.82010582]\n",
      " [ 2.79569892  5.59139785  6.23655914 85.37634409]]\n",
      "--------------------------\n",
      "Astro accuracy 73.28767123287672\n",
      "Neuron accuracy 90.97744360902256\n",
      "Oligo accuracy 90.21164021164022\n",
      "Others accuracy 85.3763440860215\n",
      "------------------------------\n",
      "Astro f1-score  72.07685138719621\n",
      "Astro precision  72.45299145299145\n",
      "Astro recall  73.58063573853048\n",
      "--------------------------\n",
      "Neuron f1-score  89.74498348578356\n",
      "Neuron precision  88.63865705111368\n",
      "Neuron recall  91.2350232509807\n",
      "--------------------------\n",
      "Oligo f1-score  89.53548422699939\n",
      "Oligo precision  89.5567234612701\n",
      "Oligo recall  89.81387927036839\n",
      "--------------------------\n",
      "Others f1-score  87.05312402468968\n",
      "Others precision  89.14814878013637\n",
      "Others recall  85.46950321531396\n",
      "--------------------------\n",
      "Macro avg precision 84.9491301863779\n",
      "Macro avg recall  85.02476036879838\n",
      "Agreement:  1292 / 1672 =>  (77.27272727272727, '%')\n",
      "Disagreement:  380 / 1672 =>  (22.727272727272727, '%')\n",
      "------------------------------\n",
      "Of the disagreements, what are they?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Others    175\n",
       "Astro     141\n",
       "Neuron     36\n",
       "Oligo      28\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.5 \n",
    "#Confusion matrix across 10 folds, WITH thresholding \n",
    "print('with thresholding (non-calibrated) ACC :',mean(t_accuracies)*100)\n",
    "print('Macro avg F1 ',mean([i['macro avg']['f1-score'] for i in t_reports])*100)\n",
    "print('Weighted avg F1 ',mean([i['weighted avg']['f1-score'] for i in t_reports])*100)\n",
    "print(\"--------------------------\")\n",
    "C_t=sum(t_confusion_matrices)\n",
    "final_cm_t =  C_t.astype('float') / C_t.sum(axis=1)[:, np.newaxis]*100\n",
    "print(C_t)\n",
    "print(final_cm_t)\n",
    "print(\"--------------------------\")\n",
    "print(\"Astro accuracy\",final_cm_t[0][0])\n",
    "print(\"Neuron accuracy\",final_cm_t[1][1])\n",
    "print(\"Oligo accuracy\",final_cm_t[2][2])\n",
    "print(\"Others accuracy\",final_cm_t[3][3])\n",
    "print('------------------------------')\n",
    "# F1-score per class: \n",
    "print('Astro f1-score ',mean([i['Astro']['f1-score'] for i in t_reports])*100)\n",
    "print('Astro precision ',mean([i['Astro']['precision'] for i in t_reports])*100)\n",
    "print('Astro recall ',mean([i['Astro']['recall'] for i in t_reports])*100)\n",
    "print(\"--------------------------\")\n",
    "print('Neuron f1-score ',mean([i['Neuron']['f1-score'] for i in t_reports])*100)\n",
    "print('Neuron precision ',mean([i['Neuron']['precision'] for i in t_reports])*100)\n",
    "print('Neuron recall ',mean([i['Neuron']['recall'] for i in t_reports])*100)\n",
    "print(\"--------------------------\")\n",
    "print('Oligo f1-score ',mean([i['Oligo']['f1-score'] for i in t_reports])*100)\n",
    "print('Oligo precision ',mean([i['Oligo']['precision'] for i in t_reports])*100)\n",
    "print('Oligo recall ',mean([i['Oligo']['recall'] for i in t_reports])*100)\n",
    "print(\"--------------------------\")\n",
    "print('Others f1-score ',mean([i['Others']['f1-score'] for i in t_reports])*100)\n",
    "print('Others precision ',mean([i['Others']['precision'] for i in t_reports])*100)\n",
    "print('Others recall ',mean([i['Others']['recall'] for i in t_reports])*100)\n",
    "print(\"--------------------------\")\n",
    "print('Macro avg precision',mean([i['macro avg']['precision'] for i in t_reports])*100)\n",
    "print('Macro avg recall ',mean([i['macro avg']['recall'] for i in t_reports])*100)\n",
    "\n",
    "# Checking on disagreements \n",
    "\n",
    "\n",
    "thresholded_preds = pd.concat([pd.DataFrame(i) for i in y_preds_t])\n",
    "thresholded_preds = thresholded_preds.rename(columns={0:'t_Class'})\n",
    "thresholded_preds = thresholded_preds.reset_index(drop=True)\n",
    "\n",
    "\n",
    "preds = pd.concat([pd.DataFrame(i) for i in y_preds])\n",
    "preds = preds.rename(columns={0:'Class'})\n",
    "preds = preds.reset_index(drop=True)\n",
    "\n",
    "truth = pd.concat([pd.DataFrame(i) for i in y_cv_test])\n",
    "truth = truth.rename(columns={'Class':'Truth'})\n",
    "truth = truth.reset_index(drop=True)\n",
    "\n",
    "# x_truth = pd.concat([pd.DataFrame(i[['Image','Centroid_X','Centroid_Y']]) for i in x_cv_test])\n",
    "\n",
    "#Combine absolute prediction to thresholded prediction\n",
    "\n",
    "#get predicted probabilities\n",
    "p_probs=pd.concat([pd.DataFrame(i) for i in y_prob_preds])\n",
    "p_probs= p_probs.rename(columns={0:'Astro',1:'Neuron',2:'Oligo',3:'Others'})\n",
    "p_probs = p_probs.reset_index(drop=True)\n",
    "\n",
    "results = thresholded_preds.copy()\n",
    "results.loc[:,'Class']=preds\n",
    "results.loc[:,'Truth'] = truth\n",
    "results.loc[:,'Astro'] = p_probs['Astro']\n",
    "results.loc[:,'Neuron'] = p_probs['Neuron']\n",
    "results.loc[:,'Oligo'] = p_probs['Oligo']\n",
    "results.loc[:,'Others'] = p_probs['Others']\n",
    "# results.loc[:,'Image'] = x_truth['Image']\n",
    "# results.loc[:,'Centroid_X'] = x_truth['Centroid_X']\n",
    "# results.loc[:,'Centroid_Y'] = x_truth['Centroid_Y']\n",
    "\n",
    "\n",
    "#Calculate agreement between the two \n",
    "results.loc[:,'agreement'] = (results['t_Class']==results['Class'])*1\n",
    "agreements = results['agreement'].value_counts()\n",
    "print('Agreement: ',agreements[1],'/',agreements[1]+agreements[0],'=> ',(agreements[1]/(agreements[1]+agreements[0])*100,'%') )\n",
    "print('Disagreement: ',agreements[0],'/',agreements[1]+agreements[0],'=> ',(agreements[0]/(agreements[1]+agreements[0])*100,'%') )\n",
    "print('------------------------------')\n",
    "# Of those disagreed, what are they? (those with prob < 0.5)\n",
    "print('Of the disagreements, what are they?')\n",
    "disagreed = results[results['agreement']==0]\n",
    "disagreed['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Others       446\n",
       "Neuron       411\n",
       "Oligo        381\n",
       "Ambiguous    284\n",
       "Astro        150\n",
       "Name: t_Class, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholded_preds['t_Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_Class</th>\n",
       "      <th>Class</th>\n",
       "      <th>Truth</th>\n",
       "      <th>Astro</th>\n",
       "      <th>Neuron</th>\n",
       "      <th>Oligo</th>\n",
       "      <th>Others</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.008566</td>\n",
       "      <td>0.824747</td>\n",
       "      <td>0.113488</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Others</td>\n",
       "      <td>Others</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.029127</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.072450</td>\n",
       "      <td>0.882764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neuron</td>\n",
       "      <td>Neuron</td>\n",
       "      <td>Neuron</td>\n",
       "      <td>0.353800</td>\n",
       "      <td>0.532260</td>\n",
       "      <td>0.028031</td>\n",
       "      <td>0.085910</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>0.023378</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.847142</td>\n",
       "      <td>0.126006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Astro</td>\n",
       "      <td>Astro</td>\n",
       "      <td>0.664605</td>\n",
       "      <td>0.165337</td>\n",
       "      <td>0.053372</td>\n",
       "      <td>0.116687</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     t_Class   Class   Truth     Astro    Neuron     Oligo    Others  \\\n",
       "0      Oligo   Oligo   Oligo  0.053200  0.008566  0.824747  0.113488   \n",
       "1     Others  Others  Others  0.029127  0.015659  0.072450  0.882764   \n",
       "2     Neuron  Neuron  Neuron  0.353800  0.532260  0.028031  0.085910   \n",
       "3      Oligo   Oligo   Oligo  0.023378  0.003473  0.847142  0.126006   \n",
       "4  Ambiguous   Astro   Astro  0.664605  0.165337  0.053372  0.116687   \n",
       "\n",
       "   agreement  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_Class</th>\n",
       "      <th>Class</th>\n",
       "      <th>Truth</th>\n",
       "      <th>Astro</th>\n",
       "      <th>Neuron</th>\n",
       "      <th>Oligo</th>\n",
       "      <th>Others</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Astro</td>\n",
       "      <td>Astro</td>\n",
       "      <td>0.664605</td>\n",
       "      <td>0.165337</td>\n",
       "      <td>0.053372</td>\n",
       "      <td>0.116687</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Others</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>0.298695</td>\n",
       "      <td>0.038325</td>\n",
       "      <td>0.300075</td>\n",
       "      <td>0.362905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.067006</td>\n",
       "      <td>0.014080</td>\n",
       "      <td>0.703508</td>\n",
       "      <td>0.215405</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Others</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.057702</td>\n",
       "      <td>0.364026</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.573747</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Astro</td>\n",
       "      <td>Astro</td>\n",
       "      <td>0.661711</td>\n",
       "      <td>0.191389</td>\n",
       "      <td>0.036769</td>\n",
       "      <td>0.110131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Astro</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>0.490975</td>\n",
       "      <td>0.069880</td>\n",
       "      <td>0.092155</td>\n",
       "      <td>0.346990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Others</td>\n",
       "      <td>Neuron</td>\n",
       "      <td>0.238278</td>\n",
       "      <td>0.167863</td>\n",
       "      <td>0.039806</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Astro</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>0.368320</td>\n",
       "      <td>0.032069</td>\n",
       "      <td>0.261882</td>\n",
       "      <td>0.337729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Astro</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>0.410179</td>\n",
       "      <td>0.044015</td>\n",
       "      <td>0.138716</td>\n",
       "      <td>0.407090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Others</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>0.251418</td>\n",
       "      <td>0.044808</td>\n",
       "      <td>0.122120</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>464 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        t_Class   Class   Truth     Astro    Neuron     Oligo    Others  \\\n",
       "4     Ambiguous   Astro   Astro  0.664605  0.165337  0.053372  0.116687   \n",
       "5     Ambiguous  Others   Oligo  0.298695  0.038325  0.300075  0.362905   \n",
       "7         Oligo   Oligo  Others  0.067006  0.014080  0.703508  0.215405   \n",
       "10    Ambiguous  Others  Others  0.057702  0.364026  0.004525  0.573747   \n",
       "13    Ambiguous   Astro   Astro  0.661711  0.191389  0.036769  0.110131   \n",
       "...         ...     ...     ...       ...       ...       ...       ...   \n",
       "1655  Ambiguous   Astro   Oligo  0.490975  0.069880  0.092155  0.346990   \n",
       "1660  Ambiguous  Others  Neuron  0.238278  0.167863  0.039806  0.554054   \n",
       "1662  Ambiguous   Astro   Oligo  0.368320  0.032069  0.261882  0.337729   \n",
       "1665  Ambiguous   Astro   Oligo  0.410179  0.044015  0.138716  0.407090   \n",
       "1668  Ambiguous  Others   Oligo  0.251418  0.044808  0.122120  0.581654   \n",
       "\n",
       "      agreement  \n",
       "4             0  \n",
       "5             0  \n",
       "7             1  \n",
       "10            0  \n",
       "13            0  \n",
       "...         ...  \n",
       "1655          0  \n",
       "1660          0  \n",
       "1662          0  \n",
       "1665          0  \n",
       "1668          0  \n",
       "\n",
       "[464 rows x 8 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['t_Class']!=results['Truth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test =pd.concat(x_cv_test)\n",
    "x_test=x_test.reset_index(drop=True)\n",
    "x_test_subset=x_test[['Image','Centroid_X','Centroid_Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_=results.copy()\n",
    "results_=results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ = results_.join(x_test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_Class</th>\n",
       "      <th>Class</th>\n",
       "      <th>Truth</th>\n",
       "      <th>Astro</th>\n",
       "      <th>Neuron</th>\n",
       "      <th>Oligo</th>\n",
       "      <th>Others</th>\n",
       "      <th>agreement</th>\n",
       "      <th>Image</th>\n",
       "      <th>Centroid_X</th>\n",
       "      <th>Centroid_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.008566</td>\n",
       "      <td>0.824747</td>\n",
       "      <td>0.113488</td>\n",
       "      <td>1</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10595.8</td>\n",
       "      <td>8928.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Others</td>\n",
       "      <td>Others</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.029127</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.072450</td>\n",
       "      <td>0.882764</td>\n",
       "      <td>1</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10681.8</td>\n",
       "      <td>8947.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neuron</td>\n",
       "      <td>Neuron</td>\n",
       "      <td>Neuron</td>\n",
       "      <td>0.353800</td>\n",
       "      <td>0.532260</td>\n",
       "      <td>0.028031</td>\n",
       "      <td>0.085910</td>\n",
       "      <td>1</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10713.9</td>\n",
       "      <td>9006.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>0.023378</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.847142</td>\n",
       "      <td>0.126006</td>\n",
       "      <td>1</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10706.4</td>\n",
       "      <td>9014.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Astro</td>\n",
       "      <td>Astro</td>\n",
       "      <td>0.664605</td>\n",
       "      <td>0.165337</td>\n",
       "      <td>0.053372</td>\n",
       "      <td>0.116687</td>\n",
       "      <td>0</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10805.9</td>\n",
       "      <td>9069.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     t_Class   Class   Truth     Astro    Neuron     Oligo    Others  \\\n",
       "0      Oligo   Oligo   Oligo  0.053200  0.008566  0.824747  0.113488   \n",
       "1     Others  Others  Others  0.029127  0.015659  0.072450  0.882764   \n",
       "2     Neuron  Neuron  Neuron  0.353800  0.532260  0.028031  0.085910   \n",
       "3      Oligo   Oligo   Oligo  0.023378  0.003473  0.847142  0.126006   \n",
       "4  Ambiguous   Astro   Astro  0.664605  0.165337  0.053372  0.116687   \n",
       "\n",
       "   agreement       Image  Centroid_X  Centroid_Y  \n",
       "0          1  721703.svs     10595.8      8928.6  \n",
       "1          1  721703.svs     10681.8      8947.5  \n",
       "2          1  721703.svs     10713.9      9006.9  \n",
       "3          1  721703.svs     10706.4      9014.4  \n",
       "4          0  721703.svs     10805.9      9069.3  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "755504.svs    320\n",
       "755508.svs    282\n",
       "721703.svs    257\n",
       "747848.svs    174\n",
       "721770.svs    171\n",
       "747377.svs    168\n",
       "747847.svs    158\n",
       "747813.svs    142\n",
       "Name: Image, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_['Image'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_Class</th>\n",
       "      <th>Class</th>\n",
       "      <th>Truth</th>\n",
       "      <th>Astro</th>\n",
       "      <th>Neuron</th>\n",
       "      <th>Oligo</th>\n",
       "      <th>Others</th>\n",
       "      <th>agreement</th>\n",
       "      <th>Image</th>\n",
       "      <th>Centroid_X</th>\n",
       "      <th>Centroid_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Astro</td>\n",
       "      <td>Astro</td>\n",
       "      <td>0.664605</td>\n",
       "      <td>0.165337</td>\n",
       "      <td>0.053372</td>\n",
       "      <td>0.116687</td>\n",
       "      <td>0</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10805.9</td>\n",
       "      <td>9069.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Others</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>0.298695</td>\n",
       "      <td>0.038325</td>\n",
       "      <td>0.300075</td>\n",
       "      <td>0.362905</td>\n",
       "      <td>0</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10809.4</td>\n",
       "      <td>9102.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.067006</td>\n",
       "      <td>0.014080</td>\n",
       "      <td>0.703508</td>\n",
       "      <td>0.215405</td>\n",
       "      <td>1</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10816.7</td>\n",
       "      <td>9182.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Others</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.057702</td>\n",
       "      <td>0.364026</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>0.573747</td>\n",
       "      <td>0</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10885.0</td>\n",
       "      <td>9224.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ambiguous</td>\n",
       "      <td>Astro</td>\n",
       "      <td>Astro</td>\n",
       "      <td>0.661711</td>\n",
       "      <td>0.191389</td>\n",
       "      <td>0.036769</td>\n",
       "      <td>0.110131</td>\n",
       "      <td>0</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10963.0</td>\n",
       "      <td>9249.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      t_Class   Class   Truth     Astro    Neuron     Oligo    Others  \\\n",
       "4   Ambiguous   Astro   Astro  0.664605  0.165337  0.053372  0.116687   \n",
       "5   Ambiguous  Others   Oligo  0.298695  0.038325  0.300075  0.362905   \n",
       "7       Oligo   Oligo  Others  0.067006  0.014080  0.703508  0.215405   \n",
       "10  Ambiguous  Others  Others  0.057702  0.364026  0.004525  0.573747   \n",
       "13  Ambiguous   Astro   Astro  0.661711  0.191389  0.036769  0.110131   \n",
       "\n",
       "    agreement       Image  Centroid_X  Centroid_Y  \n",
       "4           0  721703.svs     10805.9      9069.3  \n",
       "5           0  721703.svs     10809.4      9102.1  \n",
       "7           1  721703.svs     10816.7      9182.5  \n",
       "10          0  721703.svs     10885.0      9224.1  \n",
       "13          0  721703.svs     10963.0      9249.8  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect = results_[results_['t_Class']!=results_['Truth']]\n",
    "incorrect.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorrect_as_file = incorrect[['Image','Truth','Centroid_X','Centroid_Y','t_Class']]\n",
    "# path_ = 'D:/Tanrada_classification/imbalance_cortical_training/cortical_full_slide_predictions/Probabilistic_classification/Model8_occipital_classifier/cell_inspection/incorrect.txt'\n",
    "# incorrect_as_file.to_csv(path_, sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_Class</th>\n",
       "      <th>Class</th>\n",
       "      <th>Truth</th>\n",
       "      <th>Astro</th>\n",
       "      <th>Neuron</th>\n",
       "      <th>Oligo</th>\n",
       "      <th>Others</th>\n",
       "      <th>agreement</th>\n",
       "      <th>Image</th>\n",
       "      <th>Centroid_X</th>\n",
       "      <th>Centroid_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>0.053200</td>\n",
       "      <td>0.008566</td>\n",
       "      <td>0.824747</td>\n",
       "      <td>0.113488</td>\n",
       "      <td>1</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10595.8</td>\n",
       "      <td>8928.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Others</td>\n",
       "      <td>Others</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.029127</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.072450</td>\n",
       "      <td>0.882764</td>\n",
       "      <td>1</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10681.8</td>\n",
       "      <td>8947.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Neuron</td>\n",
       "      <td>Neuron</td>\n",
       "      <td>Neuron</td>\n",
       "      <td>0.353800</td>\n",
       "      <td>0.532260</td>\n",
       "      <td>0.028031</td>\n",
       "      <td>0.085910</td>\n",
       "      <td>1</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10713.9</td>\n",
       "      <td>9006.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>Oligo</td>\n",
       "      <td>0.023378</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.847142</td>\n",
       "      <td>0.126006</td>\n",
       "      <td>1</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10706.4</td>\n",
       "      <td>9014.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Others</td>\n",
       "      <td>Others</td>\n",
       "      <td>Others</td>\n",
       "      <td>0.084398</td>\n",
       "      <td>0.024933</td>\n",
       "      <td>0.139737</td>\n",
       "      <td>0.750933</td>\n",
       "      <td>1</td>\n",
       "      <td>721703.svs</td>\n",
       "      <td>10864.0</td>\n",
       "      <td>9153.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  t_Class   Class   Truth     Astro    Neuron     Oligo    Others  agreement  \\\n",
       "0   Oligo   Oligo   Oligo  0.053200  0.008566  0.824747  0.113488          1   \n",
       "1  Others  Others  Others  0.029127  0.015659  0.072450  0.882764          1   \n",
       "2  Neuron  Neuron  Neuron  0.353800  0.532260  0.028031  0.085910          1   \n",
       "3   Oligo   Oligo   Oligo  0.023378  0.003473  0.847142  0.126006          1   \n",
       "6  Others  Others  Others  0.084398  0.024933  0.139737  0.750933          1   \n",
       "\n",
       "        Image  Centroid_X  Centroid_Y  \n",
       "0  721703.svs     10595.8      8928.6  \n",
       "1  721703.svs     10681.8      8947.5  \n",
       "2  721703.svs     10713.9      9006.9  \n",
       "3  721703.svs     10706.4      9014.4  \n",
       "6  721703.svs     10864.0      9153.5  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = results_[results_['t_Class']==results_['Truth']]\n",
    "correct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_as_file = correct[['Image','Truth','Centroid_X','Centroid_Y','t_Class']]\n",
    "# path_ = 'D:/Tanrada_classification/imbalance_cortical_training/cortical_full_slide_predictions/Probabilistic_classification/Model8_occipital_classifier/cell_inspection/correct.txt'\n",
    "# correct_as_file.to_csv(path_, sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
